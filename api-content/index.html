{"posts":[{"title":"Esrally性能测试步骤与调优","content":" esrally一个对ElasticSearch做基准性能测试的工具，是 elastic 官方开源的一款基于 python3 实现的针对ES 的压测工具，ES官方也是基于 esrally 进行 es 的性能测试。 安装部署 python3.8安装 使用root用户操作： yum install libffi-devel tar zxvf Python-3.8.0.tgz cd Python-3.8.0 ./configure --prefix=/usr/local/python3 make -j120 make install rm -rf /usr/bin/python3 ln -s /usr/local/python3/bin/python3 /usr/bin/python3 ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3 可以自行查阅python官网：https://www.python.org/ 或者直接执行下载命令： wget https://www.python.org/ftp/python/3.8.0/Python-3.8.0.tgz git安装 本环境已安装git version 2.23.0，如果git不满足要求，参考以下内容： git 源码可以通过下面的链接获取，有各种版本： https://mirrors.edge.kernel.org/pub/software/scm/git/ 卸载旧版本 yum remove git 编译安装 tar -xzvf git-2.23.0.tar.gz cd git-2.23.0 ./configure --prefix=/usr/local/git --with-openssl=/usr/local/openssl sudo make &amp;&amp; make install 配置GIT的环境变量去修改 /etc/profile export GIT_HOME=/usr/local/git-2.23.0 export PATH=\\$PATH:\\$GIT_HOME 保存后执行source /etc/pfofile 使用 git version 查看GIT版本。 在线安装esrally pip3 install esrally==2.0.2 -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cn 错误1 ERROR: botocore 1.13.50 has requirement urllib3&lt;1.26,&gt;=1.20; python_version &gt;= &quot;3.4&quot;, but you'll have urllib3 1.26.9 which is incompatible. 解决办法： pip3 install urllib3==1.25 重新执行安装命令，显示所有依赖均满足条件。 错误2 [root@worker1 opt]# esrally configure -bash: esrally：未找到命令 不是安装失败，是因为esrally命令不会自动加入了环境变量！ cd /usr/local/python3/bin ./esrally configure 错误3 最后一行报错ImportError: cannot import name ‘soft_unicode‘ from ‘markupsafe‘…… 查到弃用警告：“soft_unicode”已重命名为“soft_str”.旧名称将在 MarkupSafe 2.1 中删除，刚好这里已经是2.1.1版本了，看来需要回退低版本。 解决办法： pip3 install markupsafe==2.0.1 再次执行./esrally configure成功安装了！ 测试命令 数据集下载 这部分测试所需要的数据集可以在测试中下载，但数据集大且网络不稳定，因此预下载。 如下操作： # downloads the script from Github curl -O https://raw.githubusercontent.com/elastic/rally-tracks/master/download.sh chmod u+x download.sh chown elasticsearch download.sh # download all data for the geonames track su elasticsearch cd /usr/local/python3/bin/ ./esrally configure cd ~ ./download.sh geonames 测试命令 geonames测试实例： ./esrally --pipeline=benchmark-only --target-hosts=192.168.1.104:9200,192.168.1.106:9200,192.168.1.108:9200 --track=geonames --offline --track-params=&quot;number_of_shards:32,bulk_indexing_clients:128&quot; --report-file=/opt/report_geonames_32_128-numa.csv --report-format=csv 性能调优 ES参数 ES的内存（heap size） 堆内存配置文件 jvm.options设置(相同大小，不超过32g情况下尽可能大)： -Xms16g -Xmx16g ES的thread_pool 主要是不超过物理机的cpu核心数，一般越大越好。 注意！这类参数现在不能动态设置了，必须直接修改es的配置文件保存然后重启ES集群。 关闭ES的监控（xpack），设置xpack.monitoring.collection.enabled 为false，提高稳定性。 调整ES的数据盘为多目录 对ES读性能影响不大，可能略微对写ES性能有影响。 esrally参数 主要是分片数（shards）和客户端并发数需要调整！ 客户端并发数非常重要！客户端并发数非常重要！客户端并发数非常重要！ 如果不调大这个clients数其他参数基本白搭。 踩坑2天…… 参考文档 01：Elasticsearch压测之Esrally压测标准 - 腾讯云开发者社区-腾讯云 02：esrally：Elasticsearch 官方压测工具及运用详解-阿里云开发者社区 03：es7.3的性能参数调优_thread_pool.write.size ","link":"https://kelvin-qin.github.io/post/esrally-xing-neng-ce-shi-bu-zou/"},{"title":"ES集群异常修复与进阶问题","content":"failed to execute pipeline for a bulk request问题 客户使用场景是使用第三方工具大批量往ES写入数据，报错写入失败，需排查。 已知情况： 当前环境各组件运行正常，数据由HBase侧不停写入ES 索引主分片数10个，副本1个 ES HEAP设置是8GB，JVM内存资源较为紧张 问题分析： 查报错日志推断是写入侧推送数据过快，ES处理能力不足导致！ 解决办法： 常规场景下应该下调往ES推送的数据大小或频率。 增大ES的内存，现场距离ES内存峰值30GB仍有很大空间。 考虑线程池过小导致，参考后面章节。 实际处理： 略微下调主分片个数使其均匀分布（10 -&gt; 8），将索引的副本数从1减为0，相当于写入压力减少一半，至此现场问题解决，没再出现上述报错。 磁盘I/O异常引起的问题修复 用户在例行巡检过程发现es有data节点处于停止状态，进行启动后，页面进度条显示绿色已完成，但实际进程启动失败了。 问题排查与处理： 排查节点日志，发现不能访问数据路径，报错关键内容如下： Unable to access ‘path.data’ (/mnt/elasticsearch/slave/data)…… 检查挂载磁盘，使用命令df -Th或lsblk -f 发现磁盘没有占满，使用容量还有剩余很多，文件系统为xfs。 cd到数据路径，没有报错，但ls有报错：ls: cannot open directory Input/output error。据此怀疑磁盘或文件系统故障。我们先排除文件系统故障。 此时设备已经处于不可用状态，先尝试重启机器（reboot）-&gt; 就这样解决了。 如果重启没有解决，依然是Input/output error，那么尝试进行文件系统修复操作如下： 从【2】中获取到要修复的挂载点，如/dev/sdb xfs的文件系统，使用如下命令进行修复:xfs_repair /dev/sdb 正常的话要提示“设备或资源忙”,&quot;couldn't initialize XFS library&quot;，需要先取消挂载umount /dev/sdb 继续执行修复： xfs_repair /dev/sdb -L Phase 1 - find and verify superblock... Phase 2 - using internal log - zero log... Phase 4 - check for duplicate blocks... - setting up duplicate extent list... - check for inodes claiming duplicate blocks... - agno = 0 - agno = 3 - agno = 4 - agno = 2 - agno = 5 - 【注：有剪辑省略内容】 Phase 5 - rebuild AG headers and trees... - reset superblock... Phase 6 - check inode connectivity... - resetting contents of realtime bitmap and summary inodes - traversing filesystem ... - traversal finished ... - moving disconnected inodes to lost+found ... Phase 7 - verify and correct link counts... done 修复完成后再把磁盘挂上，即可生效：mount /dev/sdb /mnt/elasticsearch/data。经实测，数据没有丢失，也就是不会使情况更糟——要么修好了，要么修不好保持原样。 ps：这个问题不是ES自身引起的，而是外界干扰，不要浪费太多时间去处理。如果修复失败，报修硬件。 http请求长度超过限制 最近（22年10月）接用户反馈，使用ES查询的索引表越来越多了，出现“400 Bad Request” 问题描述： ElasticSearch exception：too_long_frame_exception，reason：An HTTP line is larger than 4096 bytes。 ElasticsearchStatusException[Elasticsearch exception [type=too_long_frame_exception, reason=An HTTP line is larger than 4096 bytes.]] at org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:177) at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:526) 解决办法： 设置ES参数（在elasticserach.yml中）（http.max_initial_line_length默认4kb，具体大小根据自己实际情况设置）： http.max_initial_line_length: 8k http.max_header_size: 16k http.max_content_length: 500mb &gt; 如果有集群管理，从集群页面的自定义配置中添加，保存，最后重启ES集群。 ES的线程池设置 ElasticSearch开箱即用，在生产环境一般不需要对参数进行特别调优，但在某些特殊、极端场景下（如压测）则需要对ES各项服务线程池进行设置。 问：什么时候才需要调整这些参数？ 答：除了刚才提到的压测情境，在遇到相关报错时，根据日志提示调整。 通过curl -XGET 'http://localhost:9200/_nodes/stats?pretty'接口观察thread_pool部分的统计，关注rejected的值，elasticsearch将拒绝请求（bulk HTTP状态码429）的次数累加到rejected中。 后台查询语句： curl -s -XGET &quot;http://es.ip:9200/_cat/thread_pool/write?v&amp;h=node_name,name,active,rejected,completed&quot; 问：怎么调整这些参数？ 答：2.x版本可API动态设置，在ES 5版本之后就不能动态设置这些参数了，通过修改ES主配置文件，然后重启ES生效。（&quot;reason&quot;: &quot;transient setting thread_pool.write.queue_size, not dynamically updateable&quot;） 问：最佳实践是什么？ 答：非必要不调整，独占时最大化。 参考： thread_pool.write.size：cpu核数+1 thread_pool.search.size: （最大cpu核数*3/2+1） thread_pool.write.queue_size: 1000 thread_pool.search.queue_size: 1000 ES的双网分离设置 如果在生产环境配置了管理网和业务网分离（一般分别是千兆网和万兆网），怎么设置ES比较合理？ ES关于网络的配置参数丰富繁杂（它们都不是可动态更新的），ES双网分离场景的最佳实践是ES集群内部采用万兆-业务网，对外服务则双网都通，有管理及合规要求时按照要求设置单一网络。 实践举例： 到elasticsearch.yml中进行修改，首先注释掉network.host配置。 http.host是传入 HTTP 请求绑定到的端口；transport.host是要绑定用于节点之间通信的端口。 # elasticsearch.yml中增加 http.host: 0.0.0.0 transport.host: 192,168.1.23 # transport.host填写节点的实际万兆ip，http.host固定0.0.0.0，也可修改为指定ip ES系列参考 ES集群red状态排查与恢复 ElasticSearch重要概念和常用操作 ES线程池设置 线程池-ES-官方文档 ","link":"https://kelvin-qin.github.io/post/es-ji-qun-yi-chang-xiu-fu-chuli/"},{"title":"ES集群red状态排查与恢复","content":"问题描述 ElasticSearch开箱即用，本身并没有太多需要配置、调整的参数，平时使用中最大的问题应该就是red状态的处理恢复了。现某用户使用的ES集群报health状态为red要求技术支持。我们首先看到用户提供的状态信息： { &quot;cluster_name&quot; : &quot;real_cluster&quot;, &quot;status&quot; : &quot;red&quot;, &quot;timed_out&quot; : false, &quot;number_of_nodes&quot; : 101, &quot;number_of_data_nodes&quot; : 98, &quot;active_primary_shards&quot; : 12345, &quot;active_shards&quot; : 23456, &quot;relocating_shards&quot; : 0, &quot;initializing_shards&quot; : 40, &quot;unassigned_shards&quot; : 51, &quot;delayed_unassigned_shards&quot;: 0, &quot;number_of_pending_tasks&quot; : 0, &quot;number_of_in_flight_fetch&quot;: 0, &quot;task_max_waiting_in_queue_millis&quot;: 0, &quot;active_shards_percent_as_number&quot;: 99.704321 } 上述信息后台可以通过命令获取： curl -X GET &quot;localhost:9200/_cluster/health?pretty&quot; # 如果开启Xpack了，需要带上密码访问 curl -X GET -k -u username:passwd &quot;https://localhost:9200/_cluster/health?pretty&quot; 上述GET命令也可以直接粘贴在浏览器里获得结果。 问题定位 界面观察 ​ 已知信息是生产环境实际上的ES的数据节点（data node）理论上是99个，现在是98个，master节点是3个。 ​ 用户已经反馈从管理界面上观察ES所有实例服务状态全部正常，但集群health是red，这里的差异在于管理页面是检查进程pid判断是否存活的，而ES集群内 部则需要心跳发现机制，因此Web页面显示ES状态ok，但health显示少一个ES节点，表明有一个ES的数据节点（这里称为Slave）失联了。 现在的首要任务就是找到99个es实例里谁在滥竽充数，假装活着！ 后台日志 后台先去查看ES的master的real_cluster.log，没有找到关于连接的异常信息，里面查不到ERROR。 后台再去看个ES的slave的日志real_cluster.log，直接翻到最后，发现有连接类的错误出现了。 关键内容摘要如下： xxx-slave failed to connect to xxx2-slave7 ConnectTransportException xxx2-slave7 general node connection failure ……省略很长一串at …… 这里的关键信息就是一个slave报告说连不上【xxx2-slave7】，这就找到了。 查看更多其他slave节点的日志，也都是报连不上xxx2-slave7 综上，这个ES实例的名字知道了，顺藤摸瓜，服务器节点是xxx2，实例名是slave7，这种错误一般是集群压力大，心跳通信出问题，我们需要去重启这个ES实例。 问题处理 恢复失联的那个ES实例：上一步我们已经定位到了问题节点，需要通过管理页面重启。 页面显示重启该ES Slave成功（实际上没有成），过一会儿观察该实例并未在启动状态，ES仍是red，node仍然少一个。 再次启动该ES实例，显示成功不久后又挂掉了,属于后台进程启动不久后失败，此时去后台查该实例的日志发现有报错： # 关键词 failed to bind service IOException: failed to find metadata for existing index xxx …… [locaton: xxx] 该问题处理办法是删除实例对应的manifest文件。 这个文件的位置在该ES实例的数据存储目录下，如/data/es/slave7/nodes/0/_state，其中nodes/0/_state这几个目录应该是不变的，前面的路径随配置。 这个_state下面有manifest-xxx.st文件，直接删除或者备份后删除该文件。 再次重启该ES实例，如果等一会还未加入ES集群，日志里显示该节点频繁add、remove，再次重启该实例。 观察health，好了ES的节点数完全恢复了（从98变回了99），集群状态很快从red变成yellow了。 重点观察,initializing_shards和unassigned_shards一般逐渐减小，分片正在恢复中。 &quot;initializing_shards&quot; : 40, &quot;unassigned_shards&quot; : 51, …… &quot;active_shards_percent_as_number&quot;: 99.884321 集群活跃分片百分比升高，等所有分片恢复完成，则集群会恢复green。 索引分片数据量很大时，恢复需要花费几个小时。 后续处理 如果initializing_shards减小到0了，还有未分配的分片（unassigned_shards不是0），首先应查看未分配的原因，但一般情况可以先执行reroute命令： # 尤其在报错原因里提示分配失败是因为达到最大分配次数时，可使用这个命令。 POST /_cluster/reroute?retry_failed=true&amp;pretty 其他根据explain的原因对症下药。 # 这个命令用来查看分片不分配的原因： curl -XGET -k -u name:pass https:esip:9200/_cluster/allocation/explain?pretty # 输出的内容可能很多，可以保存到文件查看。 ","link":"https://kelvin-qin.github.io/post/es-ji-qun-red-zhuang-tai/"},{"title":"Impala常见问题笔记","content":" kerberos下怎么jdbc链接（kerberos）？ jdbc url的写法： jdbc:hive2://host:21050/;principal=hive/_HOST@BIGDATA 这种写法和开启kerberos连hive的差不多，注意principal=hive/_HOST@BIGDATA是固定写法，大部分环境只需要确认@后边的BIGDATA改为自己的信息。 官网写法： jdbc:impala://host:21050/;AuthMech=1;KrbRealm=HADOOP.COM;KrbHostFQDN=host;KrbServiceName=impala KrbRealm KrbHostFQDN 修改成自己集群的。 查询报错InternalException: Memory limit exceeded: Error occurred on backend....怎么解决？ A：首先是检查impala_daemon配置内存大小，此时检查发现默认才256MB，因此几乎可以肯定是内存不足引起的，修改mem_limit为200gb，重启impala解决。 也有说impala连接数过多导致的，可以修改fe_service_threads为更大值。 Impala Shell集群kerberos环境连接？ 首先本地kinit了hive的票据，然后执行 impala-shell -i impala.node1.com -s hive -k 显示Using service name hive，Connected to xxx:21000 这里 -i 后面跟的是任意一个impala daemon的节点地址 如何刷新impala元数据？ 初次连接impala后查询报找不到某数据库，Database does not exist: xxx 需要刷新一下impala元数据：invalidate metadata //重新加载所有库中的所有表 invalidate metadata [table_name] //重新加载指定的某个库中某张表 主要用于在hive中修改了元数据。执行这条命令时它会首先清除表的缓存，然后从metastore中重新加载全部数据并缓存，消耗较大：impala-shell -i impala.node1.com -q 'invalidate table_name' todo ","link":"https://kelvin-qin.github.io/post/impala-chang-jian-wen-ti-bi-ji/"},{"title":"CAP定理20年回首：时代变了吗？","content":"前言 10年前的今天，Eric Brewer 博士发表*CAP Twelve Years Later—How the 'Rules' Have Changed*，原文上万字，翻译并摘要、附注如下： CAP Twelve Years Later 摘要 CAP理论（CAP Theorem）断言任何基于分布式数据共享系统，最多只能满足数据一致性、可用性、分区容忍性三要素中的两个要素。但是通过显式处理分区情形，系统设计师可以做到优化数据一致性和可用性，进而取得三者之间的平衡。 Eric Brewer是 UC,Berkeley 的计算机科学教授，联系方式：brewer@cs.berkeley.edu 22年前，Eric Brewer在Principles of Distributed Computing大会上发表了CAP理论。2012年，作者在Computer杂志发表了回顾。 作者认为是三选二公式有误导性,CAP把三者的相互关系过分简单化了，强调没有必要为很小的概率去牺牲A和C。对分布式系统来说分区容忍性（P）是基本要求，但可用性和一致性在系统正常工作期间是好搭档，并非是矛盾。 引入 CAP 理论的十几年里，架构师和研究者已经以它为理论基础探索了各式各样新颖的分布式系统，甚至到了滥用的程度。NoSQL 运动也将 CAP 理论当作对抗传统关系型数据库的依据。 译注：NoSQL实践中广为人知的应该就是HBase和Cassandra了。 HBase是CAP中的CP系统，即HBase是强一致性的。（在默认不开启Region replica的情况下） HBase之所以是CP系统，和底层HDFS无关。它是CP系统，是因为每个Region同时只有一台RegionServer为它服务，对一个Region所有的操作请求，都由这一台Region server来响应，自然是强一致性的，跟单机的JVM差不多。 在这台RegionServer 挂掉的时候，它管理的Region会failover到其他Region Server时，需要根据WAL-log来redo，进行redo的Region这时候是Unavailable（不可用）的，Region需要分钟级恢复来提供服务，所以HBase降低了可用性，提高了一致性。 Cassandra拥有分散式架构：任何节点都能执行任何操作，它提供了CAP原理中的AP（可用性和分区可容忍性）。 此外，MongoDB也是CP阵营的一员。 ACID（原子性、一致性、隔离性、持久性。）和 BASE 代表了两种截然相反的设计哲学，分区一致性 - 可用性分布图谱的两极。ACID 注重一致性，是数据库的传统设计思路，也有很多优点。 中文博大精深，此处英文也有异曲同工之妙💫。 ACID和BASE除了设计哲学上是大路朝天各走一边，而且在英文原意上也是冤家路窄，前者是“酸”，后者是“碱”。 出现较晚的 BASE 硬凑的感觉更明显，它是“Basically Available, Soft state, Eventually consistent（基本可用、软状态、最终一致性）”的首字母缩写。其中的软状态和最终一致性这两种技巧擅于对付存在分区的场合，并因此提高了可用性。 当然，从数学逻辑的层面上讲，只有强一致性和不一致。最终一致性等于没有一致性，或者不保证强一致，属于一致性模型的范畴。BASE的目的是弥合分区场合下A和C之间的裂痕——虽然墙体已然出现了缝隙，但表面上仍然是粉饰的平平整整。 综上，ACID在一致性的路上一往无前，BASE则试图对可用性进行改良。CAP是帽子，BASE更像是帽檐上的挂穗🎓。 CAP 与 ACID 的关系更复杂一些，也因此引起更多误解。其中一个原因是 ACID 的 C 和 A 字母所代表的概念不同于 CAP 的 C 和 A。 译注： 记得我们在学习ACID的概念时，总是会讲银行汇取款的故事来讲原子操作至关重要，所以金融系统几乎完全使用 ACID 数据库。在金钱方面，肯定有一方无法容忍不一致的发生。 总的来说，OLTP搭ACID，OLAP搭BASE已经尽善尽美了，然而，新的体系结构HTAP还是出现了，混合事务分析处理野心勃勃地要打破事务处理和分析之间的“墙”🔨————结合前面的分析来说，理论上是做不到的，只能在API层面（对用户来说透明）进行融合，底层则是缝合怪。 CAP 理论的经典解释，是忽略网络延迟的，但在实际中延迟和分区紧密相关。网络阻塞时，依靠多次尝试通信的方法来达到一致性，比如 Paxos 算法或者两阶段事务提交，但这仅是拖延了决策的时间，系统最终要做一个决定。无限期地尝试下去，本身就是选择一致性牺牲可用性的表现。 CAP 理论经常在不同方面被人误解，对于可用性和一致性的作用范围的误解尤为严重，如果用户根本获取不到服务，那么其实谈不上 C 和 A 之间做取舍。 译注： 所谓“存地失人，人地皆失；存人失地，人地皆得”。 实践中，大部分团体认为（位于单一地点的）数据中心内部是没有分区的，因此在单一数据中心之内可以选择 CA；CAP 理论出现之前，系统都默认这样的设计思路，包括传统数据库在内。然而就算可能性极低，单数据中心完全有可能出现分区的情况，一旦出现就会动摇以 CA 为取向的设计基础。最后，考虑到跨区域时出现的高延迟，在数据一致性上让步来换取更好性能的做法相对比较常见。 当代 CAP 实践**应将目标定为针对具体的应用，在合理范围内最大化数据一致性和可用性的“合力”。**这样的思路延伸为如何规划分区期间的操作和分区之后的恢复，从而启发设计师加深对 CAP 的认识，突破过去由于 CAP 理论的表述而产生的思维局限。 后语 CAP并不是牺牲三要素其一的挡箭牌，而是启发设计者就C&amp;A进行更加高效合理的架构规约，没有万能的架构，只有最合适的场景。 后面我会另文探讨一下HTAP产品以及新生的大数据软件是真的结束了这场纷争🥂，亦或是新瓶装旧酒🍷？ ","link":"https://kelvin-qin.github.io/post/cap-ding-li-20-nian-hui-gu-shi-dai-bian-liao-ma/"},{"title":"Hudi + Spark3入门第一课","content":"Hudi + Spark3入门第一课 Apache Hudi 是下一代流数据湖平台。Apache Hudi 将数仓和数据库核心功能迁移到数据湖。Hudi 提供表、 事务、高效的 upserts/deletes、高级索引、 流式摄取、数据集群/压缩优化和并发，同时将数据使用开源文件格式。 hudi 0.10.1源码编译 maven 3.5.4，spark3.1.1，配置了aliyun的maven源 hudi目录修改pom.xml中的spark version为3.1.1，本来是3.1.2的，小版本的差异不大，看自己的环境。 编译命令mvn clean package -DskipTests -Dscala-2.12 -Dspark3 编译产物在packaging下的hudi-spark-bundle目录 spark绑定jar包名称为：hudi-spark3.1.1-bundle_2.12-0.10.1.jar，大小约38M 之前的hudi 0.9.0 版本在和spark3.1一起使用时有明显的问题，可以和spark3.0.3搭配使用。当然，这在hudi的发版说明里也有提及。 hudi-spark3.1.2-bundle_2.12-0.10.1.jar hudi-spark3.0.3-bundle_2.12-0.10.1.jar 这两个包不用自己编译，可以从maven中央仓库获取,（页面很不好找，hudi得把仓库类目梳理一下了）贴一下。 # 3.1.2版本 &lt;!-- https://mvnrepository.com/artifact/org.apache.hudi/hudi-spark3.1.2-bundle --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hudi&lt;/groupId&gt; &lt;artifactId&gt;hudi-spark3.1.2-bundle_2.12&lt;/artifactId&gt; &lt;version&gt;0.10.1&lt;/version&gt; &lt;/dependency&gt; # 3.0.3版本 &lt;!-- https://mvnrepository.com/artifact/org.apache.hudi/hudi-spark3.0.3-bundle --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hudi&lt;/groupId&gt; &lt;artifactId&gt;hudi-spark3.0.3-bundle_2.12&lt;/artifactId&gt; &lt;version&gt;0.10.1&lt;/version&gt; &lt;/dependency&gt; 使用上述预编译的包，就省略自己编译的过程了。 官网发布的支持矩阵： Spark 3 Support Matrix Hudi 支持的 Spark 3 版本 0.10.0 - 0.10.1 3.1.x (default build), 3.0.x 0.7.0 - 0.9.0 3.0.x 0.6.0 and prior 不支持 可以看到hudi 0.10版本默认构建出来是spark3.1的，也可以构建spark3.0的。 测试步骤 环境信息 已安装了Spark3.1.1 已安装Hive3.1 操作系统：CentOS 7.4 Java 8 Spark3快速测试 把hudi jar拷贝到spark安装目录的jars中,例如 cp hudi-spark3.1.1-bundle_2.12-0.10.1.jar /usr/hdp/3.0.1.0-187/spark3/jars 启动spark-sql客户端看看是否正常： 因为我们已经把hudi-spark的jar放入spark的jar包加载路径中，我们无需再显式指定它。 此外，如果有报权限类的错误，可以切换有访问hive权限的用户，这里是使用hive用户执行的操作。 ./bin/spark-sql --master yarn --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' 显示 &gt; 等待输入命令即可！(这时我还没有拷贝avro的包，也没有报错。) 创建一张不分区的hudi表： 默认就是cow格式的表，默认主键是uuid，没有预聚合字段。 create table hudi_cow_nonpcf_tbl ( uuid int, name string, price double ) using hudi; 查看表：show tables，可以看到一行刚才的表名：default hudi_table0 false 给表插入2条数据： insert into hudi_table0 select 1, 'my name is kiki', 20; insert into hudi_table0 select 2, 'qiqi', 16; 查询刚才的数据： select * from hudi_table0; …… Time taken 0.361 seconds,Fetched 2 row(s) 小试牛刀，ok。 ","link":"https://kelvin-qin.github.io/post/hudi-spark3-ru-men-di-yi-ke/"},{"title":"现代数据栈，谁主沉浮?","content":"2022年已经没有人谈大数据这个概念，不是它失败了，恰恰是因为它成功了。成功技术的吊诡之处在于，它最终会被认为是理所当然，消失在背景音中。 从最近的新闻热点里，我们不难看到大数据的身影，例如大数据加持的金税四期，疫情防控下的大数据时空伴随者等等…… 随着SaaS的普及和深入，数据驱动成为共识，云计算以及云端数据仓库的发展，逐渐有了现代数据技术栈这个新的数据生态体系。现代数据栈（MDS）主要是在欧美，确切的来讲是美国近几年出现的一个称呼，我们可以把它理解为一套新的数据生态体系。 何为现代数据栈？ 现代数据技术栈通常是指构成云原生数据平台的一组技术，对比传统的数据平台，使用它们可以大大的降低复杂度，这个技术栈的构成组件不是固定的： 一个云端的数据仓库，比如Snowflake, Redshift, BigQuery或者Databricks Delta Lake 一个数据集成服务，比如Fivetran, Segment或者Airbtye 一个ELT数据转换工具，几乎确定是dbt 一个BI层，例如Looker或者Mode 一个反向ETL工具，比如Census或者Hightouch 数据栈怎样构成？ 说起现代数据栈，那不得不说经典数据栈（戏称“Hadoop民工栈”）：一个基础平台之上的存储、计算、分析、可视化、管理和安全等的组件分层图，这种架构应该是各大数据厂商都有自家的一版。相对于RDBMS的成熟老练来讲，这套系统从诞生到现在不过二十载的时间里还是如日中天，然鹅现代社会，尤其是技术生命的新陈代谢已经到了一个非常高的水平，一代人的时间足以产生翻天覆地的变化。 看到这张架构图里是否有你熟悉的面孔？ 不错，还有几个老朋友，NiFi、Spark、Flink都还有一席之地，每一层多了很多新朋友。 提取和加载： 从所有事件源（如 Web、应用程序、后端服务）收集数据，并将它们发送到数据仓库。 付费 SaaS 工具：Stitch、Fivetran 免费和开源替代品：Singer、Meltano、Airbyte 数据仓库 组织所有数据的结构化、非易失性、单一事实来源，我们可以在其中存储和查询所有数据。 付费：AWS Redshift、Google BigQuery、Snowflake 免费和开源替代品：Apache Druid 转换和建模 使用文档从原始数据创建模型以更好地使用。 付费：Dataform、DBT 免费和开源替代品：Talend Open Studio、Apache NiFi 编排 用于执行和编排处理数据流的作业的软件。 付费：Prefect.io 免费和开源替代品：Apache Airflow、Dagster 可视化和分析 为了更好地了解和解释来自不同数据源的数据。 付费：Tableau、Microsoft PowerBI、Grafana 免费和开源替代品：Metabase、D3js、DyGraphs 但是我们继续看下面这张图： Google 图片中输入“现代数据栈”即可注意到数据市场中的所有公司都在提出他们自己的技术列表来组成这个栈，虽然王婆卖瓜，但几乎所有关于现代数据栈的博客文章都没有 Spark，却无一例外地包含了dbt。 过去、现在与未来 dbt的创始人Tristan Handy表示，他从2015年开始整理&quot;现代技术栈&quot;，整整五年的时间里，构成最佳技术栈的这个集合一直维持不变。 Tristan Handy归纳的现代数据技术栈的三个不同时间段： 寒武纪大爆发第一阶段，2012-2016 现代数据技术栈是围绕着亚马逊2012年10月份发布的Amazon Redshift而产生的。 数据技术栈中其他层的核心产品的创建日期： Chartio: 2010 Looker: 2011 Mode: 2012 Periscope: 2012 Fivetran: 2012 Metabase: 2014 Stitch: 2015 Redash: 2015 dbt: 2016 你可以看到2012年的确是时代开始了。这种天地之别是由于像Redshift这种MPP(大规模并行计算)/OLAP系统的内部架构和类似于Postgres这种OLTP系统的显著的区别带来的。 简而言之，Redshift能够在巨大的数据集上响应分析查询，处理很多个数据表关联，比OLTP数据库要快10-1000倍。10-1000倍的性能提高往往会改变你对产品构建的看法。 BigQuery直到2016年才发布标准SQL的支持，因此在这之前并没有被广泛采用。Snowflake的产品直到2017-2018年才真正的成熟。 部署期，2016-2020 事实证明，这是行业要经历的一个正常的周期。一个主要的可用技术一旦发布，激发了该领域的一系列创新，然后这些产品会经历一个部署阶段使得公司去采用它们。 这是向上S曲线的一个过程，早期采用者是宽容的，但是技术需要改进才能被越来越多的受众所采纳。 这一阶段在国内基本属于经典大数据的扩张期，以及上云的兴起阶段。 寒武纪大爆发第二阶段，2020-2025 快速的做个总结，在2012年Redshift发布之后，我们立刻看到了大量的创新，释放了全新水平的性能、效率以及新的用户习惯。然后到了一个成熟期，这些新生的产品被部署到市场，精进技术，并完善功能。到现在为止，这些新产品已经准备好可以充当建立新的后续创新的基石。 我们准备迎接新的一波创新，一个新的寒武纪大爆炸。这将带来哪些类型的创新？ 数据治理 数据治理是一个时机成熟的产品领域。这个产品领域包括非常广泛的用户用例，包括数据集的发现，查看数据的血缘关系和谱系，以及为数据优先的公司内地广泛的数据使用者提供数据导航和数据痕迹。 没有很好的治理，更多的数据=更多的混乱=更少的信任。 有商用的，但缺乏广大的用户量。 Linkedin有DataHub Lyft有Amundsen Wework有Marquez Airbnb有Dataportal Spotify有Lexikon Netflix有Metacat Uber有Databook 实时 对于今天的现代数据技术栈的主要用例来讲，实时数据当然不是必须的，但是将整个数据处理流的延迟降低到15-60秒可以为这个技术解锁全新的用例。 并且我们已经有信号能够感受到这个领域的技术已经触手可及：每个主要的数据仓库开始实现支持构造更多的实时数据流。 Snowflake非常强大依赖它的流式功能，而Bigquery和Redshift都强调他们的物化视图。这两种方法都在朝着正确的方向前进，但是以我来看，这两种方法都无法让我们走到终点。 另外有趣的话题是KSQL, 一个在Kafka之上构建的流式SQL。它的确非常对有趣并且有前景，但是围绕着SQL还是存在一些限制(尤其是在表关联上)，因此对于我来讲，它还是一个“还欠点火候”的东西。 我们还没真正达到目标，但是我们已经能够看到胜利的曙光。 完成反馈闭环 数据从运营系统流入到现代数据技术栈，并在这里进行分析，支持生产业务系统，Census和Tray可以关注。 民主化数据探索 这是一个可能引起争议的观点。 今天不使用SQL的数据消费者有很多的选择。所有的主流的BI工具都有一些不用SQL就能辅助数据探索的操作界面。但是绝对没有一个(包括LookML)甚至能够远远的接近Excel的广泛采用水平或者纯粹的创作的灵活性。 这个挑战是不平凡的，如果没有一个强大的，灵活的工具能让数据消费者自服务，现代数据技术栈将永远只为少数人服务，这是一个非常糟糕的结果。 垂直分析体验 将所有的数据视为行和列的工具和专门用于某种特定数据类型的分析工具哪种更好？答案是：我们两者都需要。但是我们缺少的是在现代数据技术栈上构建的垂直的分析界面。 随着使用现代数据技术栈的公司的增加，构建像这种新的、轻量级的、垂直的应用的机会将会大大的增加。你已经可以在Looker的应用市场中看到这个方向。 关于实时 现代企业的需求和要求正在以戏剧性的方式转变。因此，旧的“批处理”模式正在让位于更细化、更高频率的实时更新，从而带来更新鲜的数据和更快的洞察力。 除了分析性的洞察力，实时数据基础设施正在促成一类新的应用，可以在数据发生变化时做出反应。这涉及到数据堆栈的每一个部分，从数据摄取，到业务分析，到机器学习和人工智能。 在现代数据栈中，实时基础设施和工具可以采取多种形式： 以高频率和高容量将小数据包从A处流向B处(例如：Apache Kafka、Redpanda、Apache Pulsar)。 通过流处理工具过滤和转换对流数据(例如：Apache Flink、Apache Samza、Decodable)。 实时分析，让分析师在低延迟的情况下获得对业务查询的最新反馈(例如：Materialize、ClickHouse、Tinybird)。 实时或在线机器学习模型，不断适应和学习数据，并实时生成预测(例如：Tecton)。 今天，将这些不同的系统组合起来仍然是是一件棘手的事情,但是，进行这些投资的组织将获得丰厚的回报。 关于数据栈民主化 数据管理简化浪潮是一个非常重要的趋势。 对于数据民主化的一个定义。总结起来就是：数据民主化是让一个组织中所有人都可以无阻碍的去访问需要的数据，从而让这个组织中的每个人都可以随时随地的基于数据去做决策而不会受到限制。 数据民主化是目前海外数据领域一个比较热门的词汇，作者认为这跟海外的社会环境以及企业发展的现状密不可分。对于国内来讲，由于我们国情的不同，数据民主化应该还有相当长的一段路要走。 现代数据栈的未来 现在采用现代数据技术栈路线的公司一般会根据需要来采用其中合适的技术-也就是你并不需要里边所有的组件，有些公司也可能会采用其他的技术，比如Airflow, Dagster或者Prefect用作编排层。一个简单的架构示例如下图： 仅仅在云上拥有一个数据平台并不能称之为一个&quot;现代数据技术栈&quot;。如果我们纵观整个生态系统中的技术，我们会发现他们有一些共同的属性，这些属性正是现代数据技术栈的核心，是作为现代数据技术栈的关键能力： 提供一个托管式的服务：不需要或者让客户尽量少的配置就能使用，并且绝对不需要工程师的介入。用户可以立即可以开始使用，这绝对不是一个乏味的营销承诺。 以云数据仓库为中心：如果公司正在使用一个流行的云端数据仓库，所有的一切前提条件就已经具备。因为已经知道数据在哪里，你就彻底消除了杂乱无序的集成，并且所有工具都可以很好的一起工作。 通过以SQL为中心的生态系统来使得数据民主化：工具是为了数据/分析工程师和业务人员所构建的。这些用户通常最了解公司的数据，因此通过为他们提供会说他们语言的工具来提高他们的技能是非常有意义的。 弹性工作负载：按照使用量付费。对于大的处理负载，可以随时进行扩容。对于现代的云来讲，金钱是限制规模的唯一因素。 专注于可以运行的工作流：点击式工具对于低技术用户来说是很好的，但是如果没有可行的路径让相关工作转移到生产，它就是毫无价值。现代数据堆栈工具通常以自动化为核心能力构建。 ps：这就像智能机时代的到来摧枯拉朽地席卷了功能机的时代，处于变革前夕的路口仍然能看到过去Nokia的辉煌，但落幕就在一夕之间，错愕、无情、又理所当然。 云数据仓库本身很有用，但不是变革性的。围绕这些平台涌现的生态系统真正使现代数据技术栈成为现实。创新者在开始将精力和资源投入到新的创新中之前，还需要看到来自用户的积极信号；否则，他们就有可能在一个死的平台上进行创新的风险。 Handy 得出的结论是，未来五年现代数据技术栈应该会出现另一轮创新，他强调了他认为已经成熟的五个关键领域值得探索：数据治理、实时、完成和反馈循环、数据访问民主化以及垂直分析体验。 在现代数据技术栈成长到今天这个时候，我相信如下的几个关键领域的创新正在成熟： 人工智能 数据分享 数据治理 流式计算 应用服务 人工智能 任何在过去十年从事数据科学相关的工作的人都可能熟悉&quot;Data Science Hierarchy of Needs&quot;, 它如下图所示： 每个步骤都是环环相扣的，该图说明了达到甚至可以开始解决 AI 问题的所需要的依赖。具象化的需求图已经呼之欲出，只缺AI层： 对于许多业务来讲，AI代表了巨大的增长机会。我们相信到 2030 年，几乎每个行业的领先公司都将把在整个企业中启用人工智能作为优先考量的事情。然而，不乏文章哀叹普通公司未能将他们的数据科学工作投入到有意义的生产环境中。现代数据技术栈是一个新型AI平台的一个重要的着陆点，这个AI平台是一个声明式的数据优先的AI平台，能够大幅度的简化操作AI的复杂度。 数据分享 数据即服务 数据只有被分享和能流动才能产生更大的价值。（Census和Hightough 反向ELT） 数据治理 回到让现代数据技术栈能够服务大型企业这个话题：数据治理的重要性再怎么强调都不为过。一些公司有如此多的数据集，如果没有数据治理工具，几乎不可能跟踪和理解他们的数据。这是一个单个的个体会经常忽略的问题-与公司总体的数据视图相比，一个人的视野通常会比较有限。但是优秀的数据领导者会始终如一的强调好的数据治理工具的重要性，这些工具通常涵盖数据发现、数据可观测性、数据目录、数据血缘以及审计等等。 在现在数据技术栈为背景的基础上，有不少公司在尝试解决这个问题。尽管这场竞赛还远未结束，但是有一些供应商表现出了巨大他的前景：Monte Carlo Data, Stemma以及Metaplance。 流式处理 今天，相关流式处理功能已经在不同的供应商的平台中提供：Snowflake有Snowpipe并且暗示了一些新的能力，BigQuery, Redshift以及Snowflake有物化视图(尽管有一些严重的限制); Databricks支持Structured Streaming。除了这些标准的CDW提供商外，Confluent提供ksqlDB, Decodable正在重新设计流式数据工程，Meterialize是一个新定位自己的现代数据技术栈原生的流式计算的公司(通过使用dbt插件)。 如果有人能够提供一种解决方案，我不需要考虑基础设施，将我的数据移动到一个新平台，并且可以通过标准 SQL 查询简单可靠地获取实时数据，这将是非常值得大书特书的。 应用服务 云端数据仓库完全属于OLAP类型，然后实际的应用经常需要高并发和低延迟，这通常是OLTP类型的特性。 拭目以待 考虑现代数据栈？ 数据湖扮演什么角色？ 数据湖仍然有一席之地，但不是今天这个样子。随着时间的推移，关系SQL数据仓库将替换数据湖。 Snowflake与MDS 「现代数据栈」是否成立并不取决于技术本身，而是背后更大的前置条件——市场是否接受公有云。 Snowflake让很多人知道了云数仓，云数仓也是MDS的核心一环，没有云数仓的MDS犹如海市蜃楼。CDW 改变了企业收集与统一数据的方式，创业公司只需要接入几家主流 CDW，以 SaaS 的形式提供服务，专注在自己核心的价值主张上，然后迅速成长并吸引到资金。 Fivetran与MDS Fivetran的使命是让用户使用数据就像用电一样简单，无论数据的来源如何。它在传统的ETL赛道上创新，独创了适合云计算时代的ELT+E模式。目前，Fivetran估值56亿美元，是数据整合赛道的领导者，与Snowflake、IBM、SAP、Oracle等数据库或数据仓库连接，与ASICS、Autodesk、DocuSign、Forever 21、WeWork等知名公司是合作伙伴。 Fivetran支持数百个数据库的连接。 如果数据是新石油， Fivetran就是从源头到炼油厂的管道。 每多一种连通，就意味着要增加一种连接器。Fivetran目前支持超过150种数据源，这是Fivetran的护城河，也是在数据整合赛道中无与伦比的竞争优势。 Airbyte的产品主要的使用场景跟Fivetran非常类似，是ELT + E的场景，关注数据的互联互通，和最终的数据交付。 回头来讲，Airbyte和Fivetran唯一的区别是：产品是否开源。 附录 1：[[DBT前调]] 2：[[ETL与ELT]] 参考链接 关于现代数据技术栈的浅见 https://www.modb.pro/db/377596 面向初创公司的现代数据堆栈 https://jiagoushi.pro/modern-data-stack-startups 现代数据栈谁可以替换Spark https://mp.weixin.qq.com/s/YPDyKohKdw0-wqzoaCSD1g Tristan Handy，2020年12月，The Modern Data Stack:Past Present, and Future 2025年的现代数据栈 https://m.meremoggies.com/blog/modern-data-stack-2025 2021 年要寻找的 6 种现代数据堆栈趋势 https://segmentfault.com/a/1190000038585639 聊一下数据民主化 https://www.modb.pro/db/377592 Jordan Volz，现代数据栈的未来 https://continual.ai/post/the-future-of-the-modern-data-stack Fivetran：云计算时代的数据管道 https://view.inews.qq.com/a/20220119A08J9Z00 ","link":"https://kelvin-qin.github.io/post/xian-dai-shu-ju-zhan-shui-zhu-chen-fu/"},{"title":"效率工具合集(2022更新)","content":" 工欲善其事，必先利其器。记录分享用到的所有好用的工具、软件、小程序，包括markdown、chrome插件、快捷工具、截图OCR等等。 MarkDown编辑器 Typora 用习惯了，虽然现在收费了，但是一些老版本（1.0之前）仍然可以使用，流畅简洁，所见即所得👁‍🗨。 MarkText Typora 替代，同样朴素👍。 MarkText是一款比Typora更简洁优雅的markdown编辑器，完全开源免费🆓。 Joplin 发现的另一个豪华版的markdown类工具，先Mark上😆。 Joplin是一个免费的开源笔记和待办事项应用程序，可以处理大量文档组织成笔记本📒。 （2022-05-21更新） 浏览器插件镜像商店 极简插件：chrome.zzzmh.cn Chrome插件地址，无需登录、无需关注、无需验证码✅ 扩展迷：extfans.com 下载时需要公众号验证码，备用☑️ Cxrdl：crxdl.com 建议你优先使用 ID值 搜索，以便获取更精确的搜索结果✔️ （2022-05-21更新） 效率工具 Quicker 从第一次接触开始，Quicker成了我系统的必备软件，安利！墙裂推荐👏 番茄钟 番茄工作法是简单易行的时间管理方法🍅 everything 搜索神器，不多说了🔎 Wox 搭配everything食用，口味更佳 🍵 图片OCR 白描 https://baimiao.uzero.cn/ 有网页版本和App版，精准程度第一🔝 闪电识字 微信小程序可快速访问，一天5次免费识图🔜 关系图/UML 作为Visio替代，可以使用亿图，但亿图现在也收费了，推荐diagrams.net🆓 ","link":"https://kelvin-qin.github.io/post/xiao-lu-gong-ju-ji-he/"},{"title":"Openlookeng的性能与Hive&Spark对比","content":"测试说明 系列用例主要是为了获取一组计算引擎的性能基准，3物理节点500G数据较为适宜，既能考验性能又不花费太长时间。 每组用例测3次（列表循环），可取平均值和最优值，各用例使用接近的总内存大小。 Hive和Spark使用YARN管理分配资源（CPU和内存），openlookeng自行管理Worker Memory。 数据准备 因为TPC-DS耗时太长，这里选择使用TPC-H的全量22 SQL测试。 生成TPC-H测试数据集，8张表，22sql 生成500GB数据，hive表，大约花费3个小时 1TB数量大概花费4小时。 组件版本说明： 组件 版本 Hive 3.1.0 Spark 3.1.1 Openlookeng 1.6.0 测试执行 这里只记述核心的命令，辅助的前后条件未记录 Hive hive -n hive -p hive --hiveconf hive.tez.cpu.vcores=3 -f /home/hdfs/tpch_allquery2-hive.sql &gt; /home/hdfs/hive-500g-101.log 2&gt;&amp;1 Openlookeng java -jar /opt/openlookeng/hetu-server-1.6.0/bin/hetu-cli-1.6.0-executable.jar --server localhost:9090 --catalog hive --ignore-errors --schema tpch_flat_parquet_500 --progress -f /home/hdfs/openlookeng22.sql &gt; /home/hdfs/olk-500g-201.log 2&gt;&amp;1 注意：Openlookeng并不能直接执行TPC-H的hive版本sql，需要改动不少，可以参照使用presto版本的或者trino的 Spark3 ./bin/spark-sql --master yarn --driver-memory 5G --driver-cores=6 --num-executors 18 --executor-cores 5 --executor-memory 25G --database tpch_flat_parquet_500 -f /home/hdfs/tpch_allquery2.sql &gt; /home/hdfs/sql22-500g-301.log 2&gt;&amp;1 测试结果 记录执行22SQL的总时间，单位为秒，结果值是记录每一条SQL的执行时间并求和得出。 Hive run-1 run-2 run-3 4135.282 4075.399 4102.043 设置core为1 增加cores为2 增加cores为3 Openlookeng run-1 run-2 run-3 2950.68 3253.20 3452 逐条SQL执行 批量执行 批量执行，同run-2 spark run-1 run-2 run-3 1665.228 1617.062 1526.685 正常 正常 正常 测试结论 500GB的TPC-H测试，各组取平均值： Hive Openlookeng Spark3 4099 3218 1602 图为标准化的性能表示，将Hive引擎的性能作为1，值越大说明性能越高！ 可见，数据量较大的情况，Openlookeng的综合性能优势不明显，Spark仍然一骑绝尘。 ","link":"https://kelvin-qin.github.io/post/openlookeng-de-xing-neng-yu-hiveandspark-dui-bi/"},{"title":"Visio/亿图 替代","content":"Visio不在Office365套装里，Mac、Linux本上本来也难以使用。 前些年可以用亿图软件（国产），但是2022年以来亿图也开始大举收费了，没购买基本无法正常使用。 亿图和Visio格式兼容，一定程度上可以替代visio创建流程图和思维导图等。 怎么办呢？ 现在隆重推荐diagrams.net（以前为draw.io），免费的在线图表软件（本地也能用）。 就这个效果足够了： 可在线使用，临时画个图就不用安装了 支持云存储 Google Drive、OneDrive、Dropbox、Github、Gitlab、Trello等💭 无需登录或注册🆓 也可以下载桌面客户端💻 draw.io可以导入.vsdx文件，兼容度可以，实际有一点点小瑕疵🔃 客户端贴心地汉化了😍，官网好像是英文的。 官网首页一瞥 开始画图 选模板 命名存为本地文件 开始画，导出多种格式 diagrams.net可以创建各种图表：流程图，思维导图，组织结构图，E-R图，信息图，网络和体系结构图，平面图，电气和机架图，UML图等。 不错不错，还要什么自行车🕶~ ","link":"https://kelvin-qin.github.io/post/visio-he-yitu-ti-dai/"},{"title":"Typora主题国内下载","content":"Typora自带了5个主题 还不够中意，但官方主题网站 似乎没办法顺畅访问🙃 解决办法✅： GitHub主题站： 【传送门🚀】 百度网盘存货📤 已下载6款主题，自取安装即可： 链接：https://pan.baidu.com/s/1AkKkJVigT1b2ba5FPuDk3Q 提取码：tytm 主题安装路径 Typora -&gt; 文件 -&gt; 偏好设置 -&gt; 外观 -&gt; 主题 -&gt; 打开主题文件夹📂 把主题css文件放在这里即可，重启Typora，变得好富有😃： ","link":"https://kelvin-qin.github.io/post/typora-zhu-ti-guo-nei-xia-zai/"},{"title":"ElasticSearch重要概念和常用操作","content":"本文记录的操作均在ElasticSearh 7.3版本全部验证过，包含基本入门操作，方便从零开始排查问题，也包含一些重要集群参数的设置和概念…… 基础的增删改查 示例以kibana命令为主，可以转换为对应的curl命令： curl -X动作 ES地址:9200/命令 [-d 'BODY'] 举例： GET查询集群状态curl -XGET localhost:9200/_cluster/health?pretty=true PUT修改集群参数设置： curl -XPUT localhost:9200/_cluster/settings -d '{ &quot;persistent&quot; : { &quot;discovery.zen.minimum_master_nodes&quot; : 2 } }' POST执行集群分片分配：curl -XPOST localhost:9200/_cluster/reroute?retry_failed=true&amp;pretty 📢 pretty=true表示格式化输出，可以单独写pretty 创建索引 # PUT直接写索引名，不要写type了 PUT index_test 写入数据 # _doc必须写，这种写法不指定文档id POST index_test/_doc { &quot;title&quot;:&quot;oppo&quot;, &quot;price&quot;:1999 } 查询数据 # 统计索引的记录条数 GET index_test/_count # 查询index_test的前5条数据 GET index_test/_search { &quot;size&quot;:5, &quot;query&quot;:{ &quot;match_all&quot;:{} } } 删除数据 # A：删除1条确定的数据 # 以下示例中的1是一个文档的id，通过上述查询可以看到。 DELETE /&lt;index&gt;/_doc/&lt;_id&gt; DELETE /index_test/_doc/1 # B：删除批量数据，通过查询删除，Delete by query API # POST /&lt;index&gt;/_delete_by_query # 例如删除某索引全部数据 POST twitter/_delete_by_query?conflicts=proceed { &quot;query&quot;: { &quot;match_all&quot;: {} } } # 删除年龄大于10的数据 POST twitter/_delete_by_query?routing=1 { &quot;query&quot;: { &quot;range&quot; : { &quot;age&quot; : { &quot;gte&quot; : 10 } } } } 批量删除的默认滚动批大小是1000，可以调整。 POST twitter/_delete_by_query?scroll_size=5000 { &quot;query&quot;: { &quot;term&quot;: { &quot;user&quot;: &quot;kimchy&quot; } } } 删除整个索引 与上述删除索引中的全部数据不同，还可以直接删除索引，拆家了这是。 # 删除index_test这个索引，逗号分隔多个索引 DELETE /index_test 索引开/关 # 某些操作需要关闭索引再进行；或者关闭一些暂时不用的索引。 POST index_test/_close POST index_test/_open # curl形式 curl -X POST &quot;localhost:9200/index_test/_open?pretty&quot; 集群运维重要操作 节点掉线后延迟分配 这个参数对于稳定ES在线集群相当重要，这个操作不影响副本分片提升为主分片。 节点离线后不会立即进行分片平衡，默认值是1m，节点离线后先等待这么久再进行副本分片分配和迁移等操作，以免频繁进行数据迁移耗费过大。 📢 transient 表示临时的，persistent表示永久的 PUT _all/_settings { &quot;settings&quot;: { &quot;index.unassigned.node_left.delayed_timeout&quot;: &quot;5m&quot; } } # 对应的curl命令如下 curl -X PUT &quot;localhost:9200/_all/_settings?pretty&quot; -H 'Content-Type: application/json' -d' { &quot;settings&quot;: { &quot;index.unassigned.node_left.delayed_timeout&quot;: &quot;5m&quot; } } ' # 但如果一个节点确实是（永远）下线了，可以指示立即分配： curl -X PUT &quot;localhost:9200/_all/_settings?pretty&quot; -H 'Content-Type: application/json' -d' { &quot;settings&quot;: { &quot;index.unassigned.node_left.delayed_timeout&quot;: &quot;0&quot; } } ' 开启关闭集群的自动分配(shard allocation) 这个在ES集群大范围重启前要考虑（滚动重启也算），维护时可以先停止分配，等维护好后再开启分配（强烈建议✅）。 # 客户端停写，关闭集群shard allocation PUT _cluster/settings { &quot;persistent&quot;: { &quot;cluster.routing.allocation.enable&quot;: &quot;none&quot; } } # 执行同步刷新 POST _flush/synced # 停止和重启ES服务后，重新开启集群shard allocation PUT _cluster/settings { &quot;persistent&quot;: { &quot;cluster.routing.allocation.enable&quot;: &quot;all&quot; } } 查看ES集群的配置 查看配置以验证上述更改是否已经生效了。（多层json格式） GET _cluser/settings 增大集群分片恢复速度 一般来说，生产环境如果执行了滚动重启，此时有大量的分片需要分配或平衡。比如，我在重启了一个20节点机器的ES集群后（该集群有2万多分片），等待数小时后还有超过1万分片未分配，但是正在初始化的分片数却只有个位数。这时就需要增大恢复速率。 📢 当然增大恢复速率会大量占用集群的磁盘I/O，网络等资源，如果业务仍在运行酌情适当增大一些，调的过大也会导致节点宕掉。 curl -X PUT &quot;es.master1.com:9200/_cluster/settings?pretty&quot; -H 'Content-Type: application/json' -d' { &quot;persistent&quot;: { &quot;cluster.routing.allocation.node_concurrent_recoveries&quot;: 50, &quot;indices.recovery.max_bytes_per_sec&quot;: &quot;500mb&quot; } } ' 上述配置将永久性地将恢复并发度调整为50，也就是一个ES节点将同时可以有50个分片进行分配，这个值默认是2；另一个值是带宽大小，默认是20mb，现在调大为500mb。 ⚠️ 注意！设置完之后，集群分片恢复速度剧增，直接导致某些实例心跳丢失，反而导致未分配数不降反增！ 此时，rest接口已无法访问，显示大量节点掉线，不要慌，节点大概率是活着的，只是通信受阻，这是立即将并发度改为10，带宽改为100mb，节点数稍后恢复正常了。 📝 此时health页面已无法访问，但实际上集群没有挂，后台设置配置显示超时，实际上成功了。 集群运维排错基本步骤 查看集群健康状态 重点是red还是yellow，未分配个数，延迟分配个数，正在初始化的个数，节点数 curl -X GET &quot;localhost:9200/_cluster/health?pretty&quot; 如果节点数不正常，那么有ES实例失联或者宕机，需要优先启动该ES实例。 （HDP/Ambari从页面启动服务。）。 ES实例全部上线后，如果分片正在初始化个数为0，且有未分配的分片，则需要explain查看未分配原因，对症下药； 如果有正在初始化的，等待恢复即可，参考集群恢复速率调整）。 一般进行处理后，分片会自动进行分配；如果未分配，可以执行reroute命令： # 尤其在报错原因里提示分配失败是因为达到最大分配次数时，可使用这个命令。 POST /_cluster/reroute?retry_failed=true&amp;pretty 重复3~5。疑难杂症的处理后续专门讲述。 ","link":"https://kelvin-qin.github.io/post/elasticsearch-gai-nian-he-cao-zuo/"},{"title":"Spark的Not Serializable：为什么对象写在main方法外就可以？","content":"问题背景 在用Scala编写spark应用程序的时候，如果在executor中要用到一些公参变量、配置或自定义样例类(如case class等等)，直接依照常规方法定义局部变量使用会报错“xxx 不可序列化（Task not serializable: java.io.NotSerializableException）”，以及对象不可序列化(object not serializable)，这时可以通过把报错变量提升到main方法外或继承Serializable来解决，but why？ 这个问题涉及java静态关键字和spark闭包的知识，研究整理如下，欢迎一起讨论。 Java的static关键字 在类中，变量的前面有修饰符static的称为静态变量（类变量），方法的前面有修饰符static的称为静态方法（类方法）。 静态方法和静态变量属于某一个类，而不属于类的对象。静态变量和静态方法在类被加载的时候就分配了内存空间 。 Scala的object与class object 在Scala中没有静态修饰符，在object下的成员全部都是静态的，可以理解为Scala把java类中的static集中放到了object对象中，定义在object中的所有成员变量和方法默认都是 static 的，但定义在main方法里的则仅仅是一个普通的局部变量。 class 在scala中，类名可以和对象名相同，该对象称为该类的伴生对象。类只会被编译加载，不能直接被执行。类和主构造器在一起被申明，主构造器会执行类定义中的所有语句。伴生对象在第一次使用的时候会调用构造器。 Spark的闭包 RDD相关操作都需要传入自定义闭包函数(closure)，如果这个函数需要访问外部变量，那么需要遵循一定的规则，否则会抛出运行时异常。闭包函数传入到节点时,需要经过下面的步骤: driver通过反射，运行时找到闭包访问的所有变量，并封装成一个对象，然后序列化该对象（serialized on the driver node） 将序列化后的对象通过网络传输到worker节点（shipped to the appropriate nodes in the cluster） worker节点反序列化闭包对象（deserialized） worker节点执行闭包函数（and finally executed on the nodes） 总之，就是要通过网络传递函数然后执行，所以被传递的变量必须可序列化，否则传递失败。本地执行时，仍然会执行上面4步。 因此解决不可序列化问题无非以下两种方法： 避免序列化： 不在map、filter等闭包内部直接引用某类（通常是当前类）的成员函数或成员变量，可以定义在map、filter等操作内部（（或使用lazy声明））或者定义在object对象中（也就是main方法外部）。 序列化它： 对相应的引用某类的成员函数或变量做好序列化处理，主要是继承序列化类（with Serializable）。 加上了lazy，相当于不对这个值进行序列化了，而是把这个隐式转换对象整个打包发送到worker上，然后用的时候才去加载。 还有，对于可以不需要序列化的成员变量可使用“@transient ”标注，被transient关键字修饰的变量不再能被序列化（慎用），一个静态变量不管是否被transient修饰，均不能被序列化。 小结 静态成员不属于对象，属于类，不能被序列化，还有瞬态的变量也不能被序列化 。 Java对象中static，transient修饰的成员不能被序列化。 static变量保存在全局数据区，在对象未实例化时就已经生成，属于类的状态。 Scala object中定义的变量默认被static修饰，所以定义在main方法外可以不被序列化（spark闭包）。 最后，节点容器中，在本地的JVM中实例化静态变量，对象加载类时直接使用它，从而绕过了网络传输那一步，最终避免了序列化错误。 ","link":"https://kelvin-qin.github.io/post/spark-de-not-serializablewei-shi-me-dui-xiang-xie-zai-main-fang-fa-wai-jiu-ke-yi/"},{"title":"SparkStreaming写Hbase速度提升5倍","content":"业务背景：使用Spark 、streaming从kafka读取数据后写入HBase。kafkaDStream是从kafka读到的一个批次的数据流。 遍历直接写入HBase 最最基础写法是直接遍历并一条一条写入hbase。 第一版的核心代码如下： kafkaDStream.foreachRDD(rdd =&gt; { if (!rdd.isEmpty()) { println(&quot;kafkaRDD get some data.&quot;) rdd.foreachPartition(partitionRecords =&gt; { // 获取HBase连接 val hbaseConnection: Connection = getHBaseConn() partitionRecords.foreach(line =&gt; { // 连接HBase表 val tableName: TableName = TableName.valueOf(ConfigLoader.getString(&quot;hbase.table.name&quot;)) val table: Table = hbaseConnection.getTable(tableName) // 将kafka的每一条消息解析为JSON格式数据 val jsonObj: Option[Any] = JSON.parseFull(line.value()) val uuid: String = UUID.randomUUID().toString // println(line.value()) val data: Map[String, Any] = jsonObj.get.asInstanceOf[Map[String, Any]] val a: String = data(&quot;a&quot;).asInstanceOf[String] val b: String = data(&quot;b&quot;).asInstanceOf[String] val c: String = data(&quot;c&quot;).asInstanceOf[String] val put = new Put(Bytes.toBytes(uuid)) val tableColumnFamily = ConfigLoader.getString(&quot;hbase.table.column.family&quot;) put.addColumn(Bytes.toBytes(tableColumnFamily), Bytes.toBytes(&quot;a&quot;), Bytes.toBytes(a)) put.addColumn(Bytes.toBytes(tableColumnFamily), Bytes.toBytes(&quot;b&quot;), Bytes.toBytes(b)) put.addColumn(Bytes.toBytes(tableColumnFamily), Bytes.toBytes(&quot;c&quot;), Bytes.toBytes(c)) // 将数据写入HBase，若出错关闭table Try(table.put(put)).getOrElse(table.close()) }) hbaseConnection.close() }) } else { println(&quot;kafkaRDD is Empty!!&quot;) } }) 这种情况下实测消费数据入库速度约2600条每秒。（每条kafka消息约1KiB） 分批写入HBase 主要变化：创建一个List[Put]，在foreach前创建一个计数器，不再每条数据提交写一次，而是计数器每10000时写一次。 具体多少条提交写一次，根据业务情况改变。 主要是开头和结尾提交时变化，伪代码如下： var listPut = new ArrayList[Put]() var count = 0 kafkaDStream.foreachRDD(rdd =&gt; { if (!rdd.isEmpty()) { println(&quot;kafkaRDD get some data.&quot;) rdd.foreachPartition(partitionRecords =&gt; { // 获取HBase连接 val hbaseConnection: Connection = getHBaseConn() partitionRecords.foreach(line =&gt; { // 连接HBase表 val tableName: TableName = TableName.valueOf(ConfigLoader.getString(&quot;hbase.table.name&quot;)) val table: Table = hbaseConnection.getTable(tableName) // 将kafka的每一条消息解析为JSON格式数据 val jsonObj: Option[Any] = JSON.parseFull(line.value()) val uuid: String = UUID.randomUUID().toString // println(line.value()) val data: Map[String, Any] = jsonObj.get.asInstanceOf[Map[String, Any]] val a: String = data(&quot;a&quot;).asInstanceOf[String] val b: String = data(&quot;b&quot;).asInstanceOf[String] val c: String = data(&quot;c&quot;).asInstanceOf[String] val put = new Put(Bytes.toBytes(uuid)) val tableColumnFamily = ConfigLoader.getString(&quot;hbase.table.column.family&quot;) put.addColumn(Bytes.toBytes(tableColumnFamily), Bytes.toBytes(&quot;a&quot;), Bytes.toBytes(a)) put.addColumn(Bytes.toBytes(tableColumnFamily), Bytes.toBytes(&quot;b&quot;), Bytes.toBytes(b)) put.addColumn(Bytes.toBytes(tableColumnFamily), Bytes.toBytes(&quot;c&quot;), Bytes.toBytes(c)) // 每次计数+1 count +=1 listPut.add(put) if(count % 10000 == 0){ Try(table.put(listPut)).getOrElse(table.close()) listPut.clear() count = 0 } }) Try(table.put(listPut)).getOrElse(table.close()) hbaseConnection.close() }) } else { println(&quot;kafkaRDD is Empty!!&quot;) } }) 这个可以提高入库速度，具体没有测。 使用原生批量写入方法saveAsHadoopDataset val input = kafkaDStream.flatMap(line=&gt;{ Some(line.value.toString) }) input.foreachRDD(rdd =&gt; { if (!rdd.isEmpty()) { println(&quot;kafkaRDD get some data.&quot;) if(args(0).toInt == 0){ val spark1 = SparkSession.builder().getOrCreate() val df = spark1.read.json(rdd) df.createOrReplaceTempView(&quot;temp&quot;) val ans = spark1.sql(&quot;select a,b,c from temp&quot;).rdd.map(x =&gt; { (UUID.randomUUID.toString, x.getString(0), x.getString(1),x.getString(2),) }) ans.map(line=&gt;{ val put = new Put(Bytes.toBytes(line._1)) put.addColumn(Bytes.toBytes(tableColumnFamily), Bytes.toBytes(&quot;a&quot;), Bytes.toBytes(line._2)) put.addColumn(Bytes.toBytes(tableColumnFamily), Bytes.toBytes(&quot;b&quot;), Bytes.toBytes(line._3)) put.addColumn(Bytes.toBytes(tableColumnFamily), Bytes.toBytes(&quot;c&quot;), Bytes.toBytes(line._4)) (new ImmutableBytesWritable, put) }).saveAsHadoopDataset(jobConf) } else { println(&quot;接受到:&quot;+rdd.count()) } } else { println(&quot;kafkaRDD is Empty!!&quot;) } }) 这相当于调用RDD.saveAsHadoopDataset(jobConf)，这就不需要自己去处理每多少条数据提交写一次了，后台使用直接写Hadoop File的方式。 实测速度提升到13500条数据每秒，速度是原来的5.1倍。 欢迎访问我的博客：https://kelvin-qzy.top ","link":"https://kelvin-qin.github.io/post/sparkstreaming-hbase-su-du-ti-sheng-5-bei/"},{"title":"源码编译后打RPM包步骤-spark","content":"目的 修改了Spark部分源码，编译成jar包后要替换原rpm包中的jar，并重新打成rpm包以便安装。 环境：CentOS 7.4 步骤 安装rpmbuild，rpmrebuild，rpmdevtools yum install -y rpm-build yum install -y rpmrebuild yum install -y rpmdevtools rpmrebuild官网 http://rpmrebuild.sourceforge.net/ 下载rpmrebuild RPM resource rpmrebuild http://rpmfind.net/linux/rpm2html/search.php?query=rpmrebuild 百度网盘下载rpmrebuild： 链接：https://pan.baidu.com/s/16_QY2aYSF4__BiqVCOG0kg 提取码：8mov 运行rpmdev-setuptree生成工作目录，使用rpmbuild --showrc | grep topdir查看工作目录。如果是root用户，此时生成的文件夹在/root/rpmbuild下。 注意：cd /root/rpmbuild;mkdir BUILDROOT 也可mkdir -p手动生成这些目录： mkdir -p /root/rpmbuild/{BUILD,BUILDROOT,RPMS,SOURCES,SPECS,SRPMS} 提取spec文件： rpmrebuild -p -n -s spark2.spec spark2_3_0_1_0_187-2.3.1.3.0.1.0-187.noarch.rpm 将生成的spark2.spec拷贝到SPECS目录下。 将spark2_3_0_1_0_187-2.3.1.3.0.1.0-187.noarch.rpm解压到rpmbuild/BUILDROOT目录下： rpm2cpio spark2_3_0_1_0_187-2.3.1.3.0.1.0-187.noarch.rpm |cpio –idv 你可能需要创建spark2_3_0_1_0_187-2.3.1.3.0.1.0-187目录，并将上一步解压出来的内容移动到这个目录。 将你修改代码编译成的jar包拷贝到解压出来的相应位置（删除原来同名的jar包！） 根据提取的spec文件把解压出的那些文件重新打rpm包： cd ~/rpmbuild/ rpmbuild -ba SPECS/spark2.spec 新生成的xxx.rpm包位置在/root/rpmbuild/RPMS下（使用root用户时） 这个rpm包可以安装使用了： rpm -ivh /root/rpmbuild/RPMS/xxx.rpm 备注 提取spec文件也可使用如下命令： rpmrebuild –package –notest-install –spec-only your.spec xxxx.rpm ","link":"https://kelvin-qin.github.io/post/yuan-ma-bian-yi-hou-da-rpm-bao/"},{"title":"Spark3设置HDFS jar包位置","content":"问题描述 使用spark提交任务到YARN时，在没有配置spark.yarn.archive或者spark.yarn.jars时， 看到输出的日志在输出： 备注：spark 3.1.1， HDP环境 Neither spark.yarn.jars nor spark.yarn.archive is set，falling back to uploading lib 使用spark-shell 或 spark-sql时（On Yarn）也有同样问题，导致启动很慢，物理机环境启动实测花费了28秒。解决办法如下： 上传jars hdfs dfs -mkdir /user/spark/spark3jars hdfs dfs -put /usr/hdp/3.1.0.1-187/spark3/jars /user/spark/spark3jars/ 设置Spark 编辑spark-defaults.conf 增加spark.yarn.jars hdfs:///user/spark/spark3jars/* 再次启动spark-shell已经没有那句提示，打开时间为20秒，快了8秒。 如果是spark集群，需从页面自定义spark-defaults.conf内容后重启Spark。 over~ ","link":"https://kelvin-qin.github.io/post/spark3-she-zhi-hdfs-jar/"},{"title":"docker源码编译打包指南（docker-ce）","content":" 前提条件：已安装docker. docker以每月发布一个版本的节奏进行开发。命名规则为：年份-月份-ce，其中ce表示社区版本。 本文用的Docker version 是 18.06.3-ce。 Docker CE包含了docker引擎，docker命令行以及打包工具。网上也有其他以moby或单独docker引擎打包的经验，比较之下还是docker ce编译打包最为全面和妥当，这样打包出来已经是rpm包可以直接安装，而不是需要手动替换docker的二进制包！ 下载源码 git clone https://github.com/docker/docker-ce 热心网友：docker-ce仓库废弃了，作者将cli和engine分开了 --2021年11月说 网速感人！ 走快车道： https://gitee.com/mumu7/docker-ce.git 163MiB大小的源码，下载完毕！ cd docker-ce 切换分支 git checkout -b v18.06.3-ce 3. 进入打包目录cd ./components/packaging/rpm/ 这样随后得到的是rpm包，也有另外目录可以打deb包，自取。 编译打包命令： make VERSION=18.06.3-ce ENGINE_DIR=/path-to-docker/docker-ce/components/engine CLI_DIR=/path-to-docker/docker-ce/components/cli centos-7 果然，不会那么顺利，立即编译失败:buildkit not supported by daemon! 6. 上图可见最终生成的编译指令是： docker run --rm -v /root/ qzy/docker-src/docker-ce/components/packaging/rpm:/v -w /v alpine chown -R root:root rpmbuild DOCKER_BUILDKIT=1 docker build --build-arg GO_IMAGE=golang:1.13.10-buster -t rpmbuild-centos-7/x86_64 -f centos-7/Dockerfile . 其中有DOCKER_BUILDKIT=1，那么改DOCKER_BUILDKIT=0尝试。 这个设置在rpm目录下的Makefile约16行处！保存后可以继续编译了。 原因是下载的docker不支持buildkit，所以需要关闭DOCKER_BUILDKIT.不纠结！ 安装或更新了很多（系统上的）依赖包，从github自行下载了很多代码块，进行了10分钟，报错： fatal: unable to access 'https://github.com/krallin/tini.git/': Failed connect to github.com:443; Connection timed out 似乎是网络问题，重试！ 确实是网络问题，重试两次下载tini完成，继续下载依赖。 后续又因网络问题重试了几次，编译完成： exit 0标示正常退出了。 进入docker-ce/components/packaging/rpm/rpmbuild/RPMS/x86_64查看： 拷贝到其他机器rpm安装。先安装docker-ce-cli后安装docker-ce,安装完成后使用docker version查看版本！如遇到问题请回复交流！ ","link":"https://kelvin-qin.github.io/post/docker-yuan-ma-bian-yi-da-bao-zhi-nan-docker-ce/"},{"title":"rsyslog模板-template与动态文件名","content":"介绍 模板是rsyslog的关键功能。 它们允许指定用户可能需要的任何格式。 它们还用于动态文件名生成。 注意：模板的关键元素是rsyslog属性。 rsyslog有预制的硬编码Templates 模板基本语法 template(parameters) { list-descriptions } 每一个模板都有一个name参数，用来指定模板名 type: 指定模板类型 list subtree string plugin List类型 template(name=&quot;tpl1&quot; type=&quot;list&quot;) { constant(value=&quot;Syslog MSG is: '&quot;) property(name=&quot;msg&quot;) constant(value=&quot;', &quot;) property(name=&quot;timereported&quot; dateFormat=&quot;rfc3339&quot; caseConversion=&quot;lower&quot;) constant(value=&quot;\\n&quot;) } template(name=&quot;outfmt&quot; type=&quot;list&quot; option.jsonf=&quot;on&quot;) { property(outname=&quot;message&quot; name=&quot;msg&quot; format=&quot;jsonf&quot;) constant(outname=&quot;@version&quot; value=&quot;1&quot; format=&quot;jsonf&quot;) } constant常量声明 支持带以下参数： value 常量值本身 outname 输出字段名（结构化输出） format 空着或jsonf Property 声明 该声明用于包含属性文本，它可访问所有属性。支持以下参数： name outname dateformat date.inUTC caseconversion 支持“lower” and “upper” format csv json jsonf 完整json字段 jsonr jsonfr compressspace position.from 从此位置开始获取子字符串（1是第一个位置） position.to field.delimiter spifno1stsp RFC3164模板处理的专家选项 droplastlf 删除尾随的LF（如果存在） datatype 等等 YYYY-MM-DD的表示法： property(name=&quot;timereported&quot; dateformat=&quot;year&quot;) constant(value=&quot;-&quot;) property(name=&quot;timereported&quot; dateformat=&quot;month&quot;) constant(value=&quot;-&quot;) property(name=&quot;timereported&quot; dateformat=&quot;day&quot;) Subtree 自rsyslog 7.1.4起可用 用例是首先创建一个自定义子树，然后将其包含在模板中，我们假设$msg包含各种字段，并且将从字段中提取的数据（与消息一起）作为字段内容存储： set $!usr!tpl2!msg = $msg; set $!usr!tpl2!dataflow = field($msg, 58, 2); template(name=&quot;tpl2&quot; type=&quot;subtree&quot; subtree=&quot;$!usr!tpl2&quot;) String 与传统模板语句非常相似。它具有强制性的参数字符串，其中包含要应用的模板字符串。 template(name=&quot;tpl3&quot; type=&quot;string&quot; string=&quot;%TIMESTAMP:::date-rfc3339% %HOSTNAME% %syslogtag%%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\\n&quot; ) 模板字符串是常量文本和替换变量的混合，基于字符串的模板是指定文本内容的好方法，尤其是在不需要对属性进行复杂操作的情况下。 插件 在这种情况下，模板由插件生成（称为“ strgen”或“字符串生成器”）。格式是固定的。 示例 写入文件的标准模板 template(name=&quot;FileFormat&quot; type=&quot;list&quot;) { property(name=&quot;timestamp&quot; dateFormat=&quot;rfc3339&quot;) constant(value=&quot; &quot;) property(name=&quot;hostname&quot;) constant(value=&quot; &quot;) property(name=&quot;syslogtag&quot;) property(name=&quot;msg&quot; spifno1stsp=&quot;on&quot; ) property(name=&quot;msg&quot; droplastlf=&quot;on&quot; ) constant(value=&quot;\\n&quot;) } 这等价于String： template(name=&quot;FileFormat&quot; type=&quot;string&quot; string= &quot;%TIMESTAMP% %HOSTNAME% %syslogtag%%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\\n&quot; ) 注意：模板字符串本身必须在一行上。 用于转发到远程主机的标准模板（RFC3164 mode） template(name=&quot;ForwardFormat&quot; type=&quot;list&quot;) { constant(value=&quot;&lt;&quot;) property(name=&quot;pri&quot;) constant(value=&quot;&gt;&quot;) property(name=&quot;timestamp&quot; dateFormat=&quot;rfc3339&quot;) constant(value=&quot; &quot;) property(name=&quot;hostname&quot;) constant(value=&quot; &quot;) property(name=&quot;syslogtag&quot; position.from=&quot;1&quot; position.to=&quot;32&quot;) property(name=&quot;msg&quot; spifno1stsp=&quot;on&quot; ) property(name=&quot;msg&quot;) } 等价的字符串写法： template(name=&quot;forwardFormat&quot; type=&quot;string&quot; string=&quot;&lt;%PRI%&gt;%TIMESTAMP:::date-rfc3339% %HOSTNAME% %syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%&quot; ) 注意：模板字符串内容必须在一行上。 写入MySQL数据库的标准模板 template(name=&quot;StdSQLformat&quot; type=&quot;list&quot; option.sql=&quot;on&quot;) { constant(value=&quot;insert into SystemEvents (Message, Facility, FromHost, Priority, DeviceReportedTime, ReceivedAt, InfoUnitID, SysLogTag)&quot;) constant(value=&quot; values ('&quot;) property(name=&quot;msg&quot;) constant(value=&quot;', &quot;) property(name=&quot;syslogfacility&quot;) constant(value=&quot;, '&quot;) property(name=&quot;hostname&quot;) constant(value=&quot;', &quot;) property(name=&quot;syslogpriority&quot;) constant(value=&quot;, '&quot;) property(name=&quot;timereported&quot; dateFormat=&quot;mysql&quot;) constant(value=&quot;', '&quot;) property(name=&quot;timegenerated&quot; dateFormat=&quot;mysql&quot;) constant(value=&quot;', &quot;) property(name=&quot;iut&quot;) constant(value=&quot;, '&quot;) property(name=&quot;syslogtag&quot;) constant(value=&quot;')&quot;) } 等价的字符串写法： template(name=&quot;stdSQLformat&quot; type=&quot;string&quot; option.sql=&quot;on&quot; string=&quot;insert into SystemEvents (Message, Facility, FromHost, Priority, DeviceReportedTime, ReceivedAt, InfoUnitID, SysLogTag) values ('%msg%', %syslogfacility%, '%HOSTNAME%', %syslogpriority%, '%timereported:::date-mysql%', '%timegenerated:::date-mysql%', %iut%, '%syslogtag%')&quot; ) 生成JSON template(name=&quot;outfmt&quot; type=&quot;list&quot; option.jsonf=&quot;on&quot;) { property(outname=&quot;@timestamp&quot; name=&quot;timereported&quot; dateFormat=&quot;rfc3339&quot; format=&quot;jsonf&quot;) property(outname=&quot;host&quot; name=&quot;hostname&quot; format=&quot;jsonf&quot;) property(outname=&quot;severity&quot; name=&quot;syslogseverity&quot; caseConversion=&quot;upper&quot; format=&quot;jsonf&quot; datatype=&quot;number&quot;) property(outname=&quot;facility&quot; name=&quot;syslogfacility&quot; format=&quot;jsonf&quot; datatype=&quot;number&quot;) property(outname=&quot;syslog-tag&quot; name=&quot;syslogtag&quot; format=&quot;jsonf&quot;) property(outname=&quot;source&quot; name=&quot;app-name&quot; format=&quot;jsonf&quot; onEmpty=&quot;null&quot;) property(outname=&quot;message&quot; name=&quot;msg&quot; format=&quot;jsonf&quot;) } 生成的数据大概是这样（美化格式后）： { &quot;@timestamp&quot;: &quot;2018-03-01T01:00:00+00:00&quot;, &quot;host&quot;: &quot;172.20.245.8&quot;, &quot;severity&quot;: 7, &quot;facility&quot;: 20, &quot;syslog-tag&quot;: &quot;tag&quot;, &quot;source&quot;: &quot;tag&quot;, &quot;message&quot;: &quot; msgnum:00000000:&quot; } 输出并不是美化json格式后的，对API来说没必要！ 最有用的来了： 为omfile创建动态文件名 template (name=&quot;DynFile&quot; type=&quot;string&quot; string=&quot;/var/log/system-%HOSTNAME%.log&quot;) 在rsyslog.conf中这样写： 按IP分文件夹，日期分文件名 template (name=&quot;DynFile&quot; type=&quot;string&quot; string=&quot;/data/log/%fromhost-ip%/%fromhost-ip%_%$YEAR%-%$MONTH%-%$DAY%.log&quot;) *.info;mail.none;authpriv.none;cron.none action(type=&quot;omfile&quot; dynaFile=&quot;DynFile&quot;) 动态文件名是自动化采集整理的好搭档！🤝 ","link":"https://kelvin-qin.github.io/post/rsyslog-mo-ban-template/"},{"title":"rsyslog的安装使用和basic配置","content":"rsyslog其实很强大！在现代的大数据、云计算的浪潮之下不显眼而已。 安装 sudo yum install rsyslog 不幸的是，发行版通常提供相当旧的rsyslog版本。可以通过官网下载rpm或deb包来安装新版，或使用docker容器安装。 一般来说，这句也不用执行，系统已经默认安装过了！ 安装新版本 这是rsyslog传送门 例如： cd /etc/yum.repos.d/ wget http://rpms.adiscon.com/v8-stable/rsyslog.repo yum install rsyslog 查看版本 rsyslogd -ver 别说版本还是很新的，v8.24： 包结构 核心软件包（通常简称为“ rsyslog”）-它包含所有其他软件包都需要的核心技术。它还包含诸如文件编写器或syslog转发器之类的模块，这些模块非常常用并且几乎没有依赖性。 例如对于MySQL组件来说是“ rsyslog-mysql”，对于ElasticSearch支持来说是“ rsyslog-elasticsearch”。如有疑问，建议使用发行版的软件包管理器并搜索“ rsyslog *”。 使用Docker容器 docker run -ti rsyslog/syslog_appliance_alpine 配置 通常在/etc/rsyslog.conf 配置语法有三种，见名知义的： basic 适合表示基本内容的格式，例如语句适合单行的格式。它起源于最初的syslog.conf格式，现在已经使用了几十年。 advanced 以前称为RainerScript格式，此格式最早在rsyslog v6中可用，最好在v7之后用。专门针对更高级的用例，例如转发到可能部分处于脱机状态的远程主机。 obsolete legacy 其名称所隐含的含义：它已过时，在编写新配置时不应使用。 本质上，所有需要在以$符号开头的一行上编写的内容都是旧格式。鼓励使用basic或advanced格式。 配置示例 basic模式仍被建议使用 对于大多数由简单语句组成的配置仍然建议使用该格式 # 把`mail`的info级别的日志写入到mail.log mail.info /var/log/mail.log # 把`mail`的error级别的日志使用tcp协议转发到远端server上 mail.err @@server.example.net 高级功能使用advanced格式 优势： 通过高级参数精细控制rsyslog操作 块结构容易理解 容易写 可安全地与包含文件一起使用 对于上面的例子，比如你想在远端server目标掉线时不丢数据，使用如下如法： mail.err action(type=&quot;omfwd&quot; protocol=&quot;tcp&quot; queue.type=&quot;linkedList&quot; target=&quot;server&quot;) 高端大气上档次 加载模块 module(load=&quot;module-name&quot;) 模块全局配置 module(load=&quot;imtcp&quot; maxSessions=&quot;500&quot;) 语法对比 行动链 使用 &amp; 链接出一个行动链条 *.error /var/log/errorlog &amp; @remote 完整写法： *.error { action(type=&quot;omfile&quot; file=&quot;/var/log/errorlog&quot;) action(type=&quot;omfwd&quot; target=&quot;remote&quot; protocol=&quot;udp&quot;) } 这样易于拓展，模块化不易出错。 stop阻止进一步的处理，符号为~： :msg, contains, &quot;error&quot; @remote &amp; ~ 高级写法为： :msg, contains, &quot;error&quot; { action(type=&quot;omfwd&quot; target=&quot;remote&quot; protocol=&quot;udp&quot;) stop } 或： if $msg contains &quot;error&quot; then { action(type=&quot;omfwd&quot; target=&quot;remote&quot; protocol=&quot;udp&quot;) stop } sysklogd 格式 语法为：选择器字段 （空格或TAB） 行动字段 选择器 选择器由facility 和priority组成，用.连接起来。这二者遵循syslog(3)描述。 可直接通过系统文件/usr/include/sys/syslog.h查看相应值。 facility: auth, authpriv, cron, daemon, ftp, kern, lpr, mail, mark, news, security (same as auth), syslog, user, uucp and local0 through local7. priority: debug, info, notice, warning, err, crit, alert, emerg. warn, error and panic 已被弃用！ “*”代表全部，none代表没有，“，”分隔多个设施，“;”分隔多规则 行动 行动抽象了一个“日志文件”。 常规文件 真实的文件，一般使用绝对路径 命名管道（fifos） 终端和控制台 tty 或 /dev/console 远端机器 统一收集处理 用户列表等 示例 # Store critical stuff in critical # *.=crit;kern.none /var/adm/critical ######### # Kernel messages are stored in the kernel file, # critical messages and higher ones also go # to another host and to the console # kern.* /var/adm/kernel kern.crit @finlandia kern.crit /dev/console kern.info;kern.!err /var/adm/kernel-info ######### # The tcp wrapper logs with mail.info, we display # all the connections on tty12 # mail.=info /dev/tty12 ############ # Write all mail related logs to a file except for the info priority. # mail.*;mail.!=info /var/adm/mail ######## # Log all mail.info and news.info messages to info # mail,news.=info /var/adm/info # Log info and notice messages to messages file # *.=info;*.=notice;\\ mail.none /var/log/messages ","link":"https://kelvin-qin.github.io/post/rsyslog-de-an-zhuang-shi-yong-he-basic-pei-zhi/"},{"title":"日志界的瑞士军刀-rsyslog","content":"简介 rsyslog的全称是 rocket-fast system for log，它提供了高性能，高安全功能和模块化设计。rsyslog能够接受从各种各样的来源，将其输入，输出的结果到不同的目的地。 rsyslog可以提供超过每秒一百万条消息给目标文件。 即使在远程目的地和更精细的处理下，性能通常也被认为是“惊人的”。 官网原画 特点 多线程; 多协议UDP，TCP，SSL，TLS，RELP； 直接将日志写入到数据库MySQL, PostgreSQL, Oracle 等; 强大的过滤器，实现过滤日志信息中任何部分的内容; 自定义输出格式； 适合企业级中转链； 配置文件 配置文件/etc/rsyslog.conf主要有3个部分 MODULES ：模块 GLOBAL DRICTIVES :全局设置 RULES：规则 RULES facitlity.priority Target auth #pam(可动态加载验证模块)产生的日志，认证日志 authpriv #ssh,ftp等登录信息的验证信息，认证授权认证 cron #定时任务相关 kern #内核 lpr #打印 mail #邮件 mark(syslog) #rsyslog服务内部的信息,时间标识 news #新闻组 user #用户程序产生的相关信息 uucp #unix to unix copy, unix主机之间相关的通讯 local 1~7 #自定义的日志设备 =============================================================== #priority: 级别日志级别: ===================================================================== debug #调式信息，日志信息最多 info #一般信息的日志，最常用 notice #最具有重要性的普通条件的信息 warning, warn #警告级别 err, error #错误级别，阻止某个功能或者模块不能正常工作的信息 crit #严重级别，阻止整个系统或者整个软件不能正常工作的信息 alert #需要立刻修改的信息 emerg, panic #内核崩溃等严重信息 ###从上到下，级别从低到高，记录的信息越来越少，如果设置的日志级别为err，则日志不会记录比err级别低的日志，只会记录比err更高级别的日志，也包括err本身的日志。 ===================================================================== Target： #文件, 如/var/log/messages #用户， root，*表示所有用户 #日志服务器，@@192.168.1.2 #管道 | COMMAND 一个@表示UDP传输，两个@@表示TCP传输 其他日志文件 last -num：记录登录系统成功的记录 lastb -num ：记录登录系统失败的记录 /var/log/btmp：登录当前系统的所有的失败的尝试 /var/log/wtmp：所有成功登录至当前系统的相关信息 /var/log/dmesg：系统引导过程中的日志信息 测试 使用rsyslog实现日志转发 关闭selinux 修改客户端配置文件，并启动服务 vim /etc/rsyslog.conf [root@send ~]# vim /etc/rsyslog.conf #将下面四行前的注释去掉 $ModLoad imudp $UDPServerRun 514 $ModLoad imtcp $InputTCPServerRun 514 #添加下列内容 $template myFormat,&quot;%timestamp% %fromhost-ip% %msg%\\n&quot; $ActionFileDefaultTemplate myFormat #修改接收方IP（服务端），一个@表示UDP传输，两个@@表示TCP传输 *.info;mail.none;authpriv.none;cron.none @@192.168.123.3:514 [root@send ~]# systemctl restart rsyslog 修改服务端配置文件，并重启服务 vim /etc/rsyslog.conf $ModLoad imudp $UDPServerRun 514 $ModLoad imtcp $InputTCPServerRun 514 *.info;mail.none;authpriv.none;cron.none /data/log/messages #添加以下内容 $AllowedSender tcp, 192.168.123.0/24 //允许 123.0网段内的主机以tcp协议来传输 $template Remote,&quot;/data/log/%fromhost-ip%/%fromhost-ip%_%$YEAR%-%$MONTH%-%$DAY%.log&quot; //定义模板，接受日志文件路径，区分了不同主机的日志，也可以在Action中配置！ :fromhost-ip, !isequal, &quot;127.0.0.1&quot; ?Remote //过滤server 本机的日志，且使用Remote模板！ [root@server ~]# systemctl restart rsyslog 在服务端创建/data/log目录，以接受大量日志信息，配置文件中的路径应当与该路径一致 [root@server ~]# mkdir -p /data/log [root@server ~]# touch messages [root@server ~]# systemctl restart rsyslog 验证 验证一： #在客户端的终端命令行输入： [root@client ~]# logger &quot;good good study~~~~~~~&quot; 验证二： #在客户端使用ssh协议登录系统： Last login: Fri May 10 22:11:54 2019 from 192.168.157.1 [root@client ~]# tail-f messages在服务端查看。 rsyslog支持日志转发，也支持将日志信息存储到mysql数据库，并且发送速度极快 注：每条消息均会经过所有规则，并不是唯一匹配的 Plugins（模块化插件） 只列举一些重要的，传送门 输入端 Input auditd :Linux审计系统 file :将任何标准文本文件转换为系统日志消息。 klog :从内核日志中读取消息，并将其提交给syslog引擎。 journal :提供将结构化日志消息从systemd日志导入到syslog的功能。 relp :通过可靠的RELP协议接收系统日志消息,比tcp更可靠 tcp udp 输出 elasticsearch file 文件 hdfs mysql MongoDB oracle pipe 管道 redis 还有很多插件，不一一列举了，如图： TODO 20200320 ！！日志文件模板明确用法见template笔记！ rsyslog轻松实现动态文件名-日志滚动模板 ","link":"https://kelvin-qin.github.io/post/ri-zhi-jie-de-rui-shi-jun-dao-rsyslog/"},{"title":"技术手记(1)","content":"2016~2019所遇到的零零碎碎技术相关的…… 工程类 IDEA相关 使用IDEA，创建Scala语言的Maven项目，可以先创建一个Scala项目，然后添加Maven框架支持。在项目上右键，Add Framework Support...选择Maven! 配置完MAVEN_HOME，IDEA的settings》Maven》Runner》设定VM ops为-Dmaven.multiModuleProjectDirectory=$M2_HOME IDEA的部分代码无故变红色，执行File-&gt;Invalidate Caches -&gt;Invalidate and Restart； 如果上述操作无效，删除工程中的项目名.im文件重新打开该项目 📢 在工作中可能需要变动的重要参数常量，写成配置文件xx.properties,写一个配置加载类去读取。这样把xx.properties文件放入resources文件下。这样打成jar包后解压后可修改配置！（强烈推荐！） 在使用IDEA将项目提交到SVN时候，提示svn版本低，或者没有svn.exe,下载安装VisualSVN，把 全路径写到command line(在设置》VCS》SVN。。) svn那个位置。然后一定要把svn仓库地址写正确了。 IDEA右下角can not use Subversion command line client :svn 这个错配置那里设置use command line client :../svn.exe 不能用TortoSVN的路径。 maven添加本地jar到pom文件，执行 mvn install:install-file -Dfile=lucene-queryparser-4.6.1本地.jar -DgroupId=org.apache.lucene -DartifactId=queryparser -Dversion=4.6.1 -Dpackaging=jar IDEA本机运行Spark程序时控制栏输出很多&quot;RpcRetryingCaller: Call exception ...Call to host.domain:60000 failed on connection exception: java.net.ConnectException: Connection refused: no further information&quot;;IDEA 读取Hbase数据，已连接zookeeper，但显示上面的重试信息，读取不到数据，看一下本机的hosts文件，配置成短域名，配置外网域名，不要内网的！！ Maven查询依赖关系，并导入到名为tree.txt的文件中：mvn dependency:tree -Dverbose &gt; tree.txt 查找冲突conflict！！！ 集群 CDH集群警告时钟偏差，确认ntp服务是否有有效的server可以给主节点设置好校时服务，然后其他节点crontab定时与主节点校时。注掉其他节点ntpd.conf中的相关内容。 Spark JdbcRdd读取数据时有NPE错误，检查sql语句的where条是否有not null条件。有时候看似查出的结果没有null值，但仍然报NPE错误，这时要给where条件增加not null判断，确保没有null的幽灵混进来。 使用IDEA的Maven工具打包工程时出错，target看下有没有删除干净，手动删除target文件夹后重新package或install。 elasticsearch集群断电或者机器重装后某些shard失效，或者写入数据报错； 关闭集群(实测也不用关，只关es节点就可以) --&gt; 找到不能启动的shard（根据报错信息提示shard编号，去ES存data的目录，顺藤摸瓜，找到问题节点的那个索引的那个shard编号目录进去） --&gt; 清除这些shard的translog(整个文件夹，可以先flush一下，以免丢数据) --&gt; 重启ES节点； 如果还不行重复以上过程。 重启es会临时解决这个问题，数据可以导入了,但后面新建的索引或者其他数据仍会报错，挨个重启es，reindex，refresh新建索引都不能解决translog_exception； 采用增加副本，关闭es节点删除节点es数据（data和log都删除），待es集群重分配完成后，启动es节点，让es集群重分配回来，对有问题的节点进行删等增操作，好像可以解决！ 因为这是生产集群，还配有sg，不能直接重装ES。否则会导致线上服务停止。 其实相当于滚动重装ES、滚动重装ES、滚动重装ES！！！ ES配置了远程网络词典 ElasticSearch启动报错 java.security.AccessControlException: access denied (java.net.SocketPermission host1234:8080 connect,resolve) 找到%JAVA_HOME%/jre/lib/security/java.policy 文件,添加如下内容: permission java.net.SocketPermission &quot;127.0.0.1:8080&quot;,&quot;connect,resolve&quot;; redis安装报错 进入目录make时 其一cc: command not found 这是没有安装gcc，yum install gcc即可； 其二fatal error: jemalloc/jemalloc.h: No such file or directory，这是指定make MALLOC=libc即可。原因百度。 Redis进行pv，uv计算，uv非常大时用Set结构存储是非常浪费的，如果不是非常需要精确的场景，使用Redis的HyperLogLog结构来存储uv类的计算。HyperLogLog 除了上面的 pfadd 和 pfcount 之外，还提供了第三个指令pfmerge，用于将多个pf计数值累加在一起形成一个新的pf值。pf的含义是这种数据结构发明者的首字母缩写。比如在网站中我们有两个内容差不多的页面，运营说需要这两个页面的数据进行合并。其中页面的UV访问量也需要合并，那这个时候 pfmerge 就可以派上用场了。 消费 kafka 数据时刚开始可以稳定跑一会，但是过不了几分钟就跑出此异常程序中断java.lang.IllegalStateException: No current assignment for partition xx 是因为我在集群上跑着消费程序，本地也在用相同的消费代码测试，结果就出现了同一个 groupID 在同一时刻多次消费同一个 topic，引发 offset 记录问题!!! ⁉️服务出现连接异常，如果网络时好时坏，一会儿能ping通一会儿不能，高度怀疑网络有环路！快请网工来解决吧！ Linux&amp;Shell 手动执行shell成功，crontab不执行，首先给sh脚本等加上x权限，chmod；然后给注意检查shell脚本中是否是全路径，不要用相对路径；然后给shell脚本中加上读取环境变量，类似于下面的：注意开头点和后面有空格，表示读取环境变量的:. ~/.bash_profile export HADOOP_USER_NAME=hdfs yum install mlocate； updatedb； 然后Linux就可以使用locate xxx 命令了。 切换用户后命令行变成-bash4.1-$。检查用户目录下后没有.bash_profile，那么问题就找到了，就是缺少用户环境配置文件，那么为什么需要这样文件，这就是跟linux的机制有关联了，因为在linux下每次通过useradd创建新的用户时，都会将所有的配置文件从/etc/skel复制到新用户的主目录下，一般默认在home下面的新用户主目录，而关于/etc/skel这个目录，是主要被useradd所用到。 这时只需要cp -a /etc/skel/. /home/userxx就正常了。 代码类 Spark项目 注意Spark版本和Scala版本，以及Java版本的一致性，有冲突时，修改pom.xml，添加scala的jar包依赖，比如 spark2.3 和 scala2.11+jdk 1.8。 ⚠️注意！环境变量中比如JAVA_HOME，路径不能有空格！（2017年1月） Spark写出文件出错。注意输出路径的正确性，是不是少了“/”等等。关键报错是：Can not create a path from an empty string ...... ⚠️在单机环境编写的Spark程序要去集群环境测试时，注意，切记一定要把RDD或DF的操作结果collect回来再写数据库！！ Scala的case class 把RDD 映射成DF，一定要把case class定义在方法外！！ Spark1.6.0尚不能在sql的where语句使用子查询，会报错，语法错误或不支持等。可使用left join的思想解决not in （子查询）的需求！（2016年12月） 在使用case class时提示Too many argruments,首先检查是否有和class 同名的其他变量，也就是检查赋予的类型是否是期望的类型，其次检查class的参数个数！！这个坑花了十几分钟💢。 本地变量bc要广播使用bc.value，RDD不需要广播。（2016年12月） 🐞分组后去重是在每个分组内去重的，会比 总体去重的结果集要大！！ 使用df.write.jdbc方法分布式入库时，报错java.sql.SQLException: no suitable driver 。这种错加载数据库驱动时不能用class.forName,而是在df.write.option(&quot;driver&quot;,&quot;数据库驱动jar名&quot;).jdbc:例如：someDataFrame.write .option(&quot;driver&quot;, &quot;oracle.jdbc.driver.OracleDriver&quot;) //指定使用的JDBC驱动类型 .mode(SaveMode.Append) // 指定写入模式为新增 .jdbc(oracleUrl, dbtable, prop) // 提供连接参数，并发写入数据 spark有许多提交参数，一些监听的Nullpointer错误可能是内存不足引起的（overhead.memory之类同时出现），可以在提交时参数提高内存参数，并发度设置。 Spark任运行报错Error：TreeNodeException：execute,tree......Task not serializable ...NotSerializableException：org.apache.spark.SparkContext ..... at rddToDataFrameHolder这种错误提示信息，是由于主函数main中一些过程使用了定义变量，这些变量从配置文件中加载，序列化失败，把这些变量提到main函数外层即可。 低版本的spark比如1.6.0没有saveToEs这个方法，或者说不能调用，2.0.0版本之上就可以。 用Scala写spark程序读取json文件，json内嵌一层json时 如【&quot;location&quot;: { &quot;x&quot;: &quot;123.345&quot;, &quot;y&quot;: &quot;100.123&quot;}】 get这个值应为getStruct(i).getString(0)以此类推，内嵌的一层json对象为struct类型，需进一步get子类型。 否则打印时强行转换会出现格式错误或cast异常或class异常报错；多层嵌套以此类推；环境为spark2.2.1版本scala2.11.8jdk1.8，idea2018.1免费版。 Spark2.2读取json，内嵌json对象 时也可以col名称可以点出来&quot;location.x&quot;,&quot;location.y&quot;，举例：spark.read.json(&quot;json_path&quot;) .selectExpr(&quot;uid&quot;, &quot;path&quot;, &quot;time&quot;, &quot;location.x as locationX&quot;,&quot;location.y as locationY&quot;) 可看到同时可以对列重命名。对于key的值本身含有【.】的列名，例如某个key为“name.first”,此时读取时直接反斜杠转义【点.】是不可以的，直接as命名也不识别，这是json用【name.first】保存的特殊字符，应当【&quot;name.first as name_first&quot;】这样就可以顺利读取到含有点的列名啦: selectExpr(&quot;uid&quot;,&quot;location.x as locationX&quot;,&quot;location.y as locationY&quot;, &quot;ti.me as ti_me&quot;) Spark整合Kafka，消费者程序消息流cache后进行多次print()操作时报错—— java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord 查询其一解决办法是“stream.map(record=&gt;(record.value().toString)).print”我没试过。 但既然是报序列化错，直接使用Kyro序列化来优化，先 conf..set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)； 然后注册这个不能序列化的类 conf.registerKryoClasses(Array(classOf[ConsumerRecord[String, String]]))，问题解决了，运行速度也快了20%左右😄。 import org.apache.spark.internal.Logging报错，显示红色，相关问题一说之前版本是org.apache.spark.Logging在2.0版本之后改变为内部类，只能在spark包内使用。我遇到的pom是2.4.4版本，已经在spark的包内了，重启IDEA无效，我将所有本地的2.4.4的jar删除重新导入，成功！ ETL类 sqoop导数据程序内存不足报错，beyond physical memory limits之类，修改脚本在sqoop import后面加-D mapreduce.map.memory.mb=2048......默认的是1G，报错内容1.0G of 1G physical memory used ... virtual memory used.Killing container. sqoop导数据给hdfs失败,yarn日志报错，dump之类，running beyond physical memory limits...physical memory used...virtual memory used.Killing container.这种内存不足，检查yarn-site.xml这个属性yarn.scheduler.minimum-allocation-mb默认1024，修改为2048，也就是 2G。（2017年3月） 数据库 Oracle的字符串要用单引号引用，比如to_date('2016-12-21','yyyy-mm-dd')。这是在pl/SQl中使用的！（2017年2月） ORACLE - ORA-01861: 文字与格式字符串不匹配format string：这个错误首先检查有没有把别的数据类型当做日期类型入库的，其次检查yyyyMMdd这样的日期格式是否写对了。数据库类型是Date的入库的类型是String报错是这个。改成java.sql.Date入库就可以了。 oracle数据库存clob字段，长字符串。修改字段类型时先将原来字段删除，再新增修改后的字段，类型。以免修改失败。 oracle修改表结构时，一步一步修改可以避不明错误，修改名称，描述，类型修改一次应用一次。（2017年） 踩坑类 注意通讯工具等发过来的时候乱码！！，直接发代码到对话框可能会被替换字符！ 🐞【未解决的技术异常】spark2.2 local模式读取union了30天的parquet数据（250万条左右），map成二元组直接进行distinct后每次结果都不一样，distinct后的数据量变化幅度较大。而分别读取每天的数据（根据单个值，而不是二元组）distinct后再聚合起来的结果就稳定可重现了！暂未发现问题所在，以前也有这么类似的用法，未发现异常~~ 想把两个Map内容合并起来。直接写++=编译器说无用，但不报错！（2018-10-08） Scala中三个引号（&quot;&quot;&quot;）引起来的内容如果前面加了“s”的话，里面的字符串含有“\\”反斜杠的会被编译掉，也就是最终字符串里面反斜杠不见了，这时候如果最终字符串需要里面带有反斜杠的实际要写“\\”双斜杠。如果不加“s”里面的反斜杠就能输出。json里面值是复杂字符串的，如json格式的字符串的反斜杠是要保留的，不然http传参时会有异常！！！ 🐞在百万行数据集进行日期格式化的时候，用的java的SimpleDateFormat类，偶发报错数组下标越界（13），转换的是Long类型的时间戳。检查报错的时间错无异常，采用joda时间类进行转换无误！！！ ⚠️org.apache.spark.SparkException: Cannot use map-side combining with array keys 说的是把Array数据当做key分组时Spark抛出异常，这是因为Scala的Array只是Java类型的封装，这个Scala数据的hashCode不依赖于其内容，因此用Array做key不能正确分组，改变Scala Array数据的元素值，它的hashCode不会变。 思考类 ","link":"https://kelvin-qin.github.io/post/ji-zhu-wan-hua-tong2016~2019/"},{"title":"JanusGraph常见问题与解决集锦","content":"这是一个JANUSGRAPH v0.4.0学习实践的填坑排错笔记，包括事务相关，代码打包编译等常见问题。 错误1 事务相关 问题描述 已实现在后台使用addV()添加一些顶点数据，后台控制台可以查到，但执行下面代码输出为~0 gremlin命令行可以查到，但在IDEA中运行java程序查不到结果 代码示例 public static void main(String[] args) throws Exception { GraphTraversalSource graph = traversal() .withRemote(DriverRemoteConnection.using(&quot;my.host.com&quot;,8182,&quot;g&quot;)); List&lt;Vertex&gt; ls = graph.V().has(&quot;name&quot;).toList(); System.out.println(&quot;===&quot;+graph.V().count().next()); System.out.println(&quot;===&quot;+graph.getGraph()); for(Vertex v :ls){ System.out.println(&quot;=========&quot;+v.label()+&quot;=======&quot;); } graph.close(); } 解决方案 在gremlin控制台添加顶点数据后，在java程序中查不到，结果为空，原因是任何的图操作都会自动开启一个事务。如果事务没有提交，那么操作就不会生效。需要在gremlin控制台执行事务提交后才能查询到： for (tx in graph.getOpenTransactions()) tx.commit() 事务提交后即可在Java程序中查询刚才添加的数据。简单来说就需要graph.tx().commit() 错误2 GremlinServer 问题描述 远程连接GremlinServer，报错源[g]别名未在服务器上配置 错误信息：The traversal source [g] for alias [g] is not configured on the server. 代码示例 同错误1，据说ES没有正常运行也会遇到这个错误。 解决方案 修改配置文件 vim janusgraph-hbase-es.properties 在文件末尾添加如下配置： gremlin.graph=org.janusgraph.core.JanusGraphFactory 保存后重启GremlinServer服务器,命令如下 /path_to_janus/janusgraph-0.4.0-hadoop2/bin/gremlin-server.sh stop /path_to_janus/conf/janusgraph-hbase-es.properties /path_to_janus/janusgraph-0.4.0-hadoop2/bin/gremlin-server.sh start /path_to_janus/conf/janusgraph-hbase-es.properties java程序即可远程连接GremlinServer，查询数据无误。 错误3 ES相关 问题描述 java.lang.IllegalArgumentException: Could not instantiate implementation: org.janusgraph.diskstorage.es.ElasticSearchIndex 错误信息包含：Caused by: java.lang.reflect.InvocationTargetException 在Gremlin命令行创建graph时报错java.lang.IllegalArgumentException: Could not instantiate implementation: org.janusgraph.diskstorage.es.ElasticSearchIndex 注：这在IDEA项目里用java代码访问JanusGraph时也会报同样的错但错误原因不一样。 解决方案 排查如下： 执行的代码示例 gremlin&gt; graph = JanusGraphFactory.open('/opt/janusgraph/conf/janusgraph-hbase-es.properties') 经 curl http://localhost:9200检查相应正常。但无论如何修改httpclient依赖包还是报错。经排查发现此时elasticsearch服务状态是red（因昨晚服务器全部掉电的缘故），将es的状态恢复到green之后，再次执行代码，直接就好了。随后用java代码读取数据一切正常。 可以看到:standardjanusgraph巴拉巴拉…… 错误4 加载示例图时报错 问题描述 加载示例图时报错SchemaViolationException： 报错信息： org.janusgraph.core.SchemaViolationException: Adding this property for key [~T$SchemaName] and value [rtname] violates a uniqueness constraint [SystemIndex#~T$SchemaName] 适用于其他SchemaViolationException（模式冲突）的情况推定 问题分析 指Schema冲突，[rtname]违反了唯一性约束。SchemaName这一属性是唯一的，添加的值与先前存在的值冲突。 这是由于先前已经加载过Graph of the Gods，load命令已经执行过，存储中已存在相同的数据了，再次添加数据造成。 解决办法 方案一 不需要执行GraphOfTheGodsFactory.load(graph)语句，直接进行示例查询。 gremlin&gt; g = graph.traversal() ==&gt;graphtraversalsource[standardjanusgraph[cql:[127.0.0.1]], standard] gremlin&gt; saturn = g.V().has('name', 'saturn').next() ==&gt;v[256] gremlin&gt; g.V(saturn).valueMap() ==&gt;[name:[saturn], age:[10000]] gremlin&gt; g.V(saturn).in('father').in('father').values('name') ==&gt;hercules 方案二 删除整个图【谨慎操作，不可恢复】， 包含 graph、data、schema 等都会被删除： JanusGraphFactory.drop(graph) 注意： 删除图操作之后要关掉重新进gremlin console，重新打开图再执行load。否则报错 Could not re-open management log org.janusgraph.core.JanusGraphException: Could not re-open management log 或者： 删除存储后端中的数据表，再执行GraphOfTheGodsFactory.load(graph)即可。 HBase中默认的数据表名是“janusgraph” 错误5 NoNodeException 问题描述 使用hbase后端存储，在gremlin命令行打开图的操作。 HBase服务正常，能使用hbase shell进行操作 使用gremlin open图时报错org.apache.zookeeper.KeeperException$NoNodeException 详细报错日志： java.util.concurrent.ExecutionException: org.apache.hadoop.hbase.shaded.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hbase/hbaseid at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) …… Caused by: org.apache.hadoop.hbase.shaded.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hbase/hbaseid at org.apache.hadoop.hbase.shaded.org.apache.zookeeper.KeeperException.create(KeeperException.java:111) 关键信息有： NodeException: KeeperErrorCode = NoNode for /hbase/hbaseid 解决方案 既然zookeeper和hbase服务均正常，检查Hbase的配置： &lt;property&gt; &lt;name&gt;zookeeper.znode.parent&lt;/name&gt; &lt;value&gt;/hbase-xxx&lt;/value&gt; &lt;/property&gt; 发现这里并非是报错日志指向的/hbase/hbaseid，而是/hbase-xxx目录。可以将在页面看到的zookeeper.znode.parent值是/hbase-unsecure，改为/hbase重启HBase解决，但不妥，更正规的解法是官网一个参数，storage.hbase.ext.zookeeper.znode.parent ，在配置文件把这个参数的值设置为与zookeeper.znode.parent的值相同就可以解决！ 可能hbase升级后默认配置有变动，之前是没有异常的。 错误6 使用索引后端elasticsearch启动时报ResponseException 问题 JanusGraph使用hbase-es配置 使用gremlin命令行启动，执行JanusGraphFactory.open方法时出错 报错信息： org.elasticsearch.client.ResponseException: method [GET], host [http://host1.bigdata:9200] …… 15:09:09 WARN org.janusgraph.diskstorage.es.rest.RestElasticSearchClient - Unable to determine Elasticsearch server version. Default to FIVE. org.elasticsearch.client.ResponseException: method [GET], host [http://host1.bigdata:9200], URI [/], status line [HTTP/1.1 503 Service Unavailable] &quot;cluster_uuid&quot; : &quot;_na_&quot;, Could not instantiate implementation: org.janusgraph.diskstorage.es.ElasticSearchIndex 解决方案 报错日志看到http://host1.bigdata:9200的GET访问不正常,有503服务不可用异常。 集群uuid为_na_，ES集群状态不正常，检查es的日志，发现es因为没有足够的master节点没正常运行：not enough master nodes discovered during pinging (found。 增加了es的master实例，ES集群正常运行了。重新运行gremlin，错误解决，可以正常加载图。 刚开始还以为是https的问题，这些个错误都是JanusGraph使用的索引后端状态异常引起的，容易走弯路。错5和错6都不是JanusGraph本身的问题，但在实践中躲不开，权当做个备忘。 错误7 待续 ","link":"https://kelvin-qin.github.io/post/janusgraph-chang-jian-wen-ti-yu-jie-jue-ji-jin/"},{"title":"JanusGraph+Hbase+ES在开启Kerberos下的配置","content":"开Kerberos环境下JanusGraph+Hbase+ES如何使用? 本文是在开启Kerberos的HDP集群环境中所作实践，其中Hbase受Kerberos管理，但ES不受Kerberos管理…… 修改配置文件janusgraph-hbase-es.properties 修改配置文件path-to-janusgraph/conf/janusgraph-hbase-es.properties（末尾3行为开Kerberos环境下需要特别修改或添加的参数）: storage.backend=hbase storage.hostname=zk.ip1:2181,zk.ip2:2181,zk.ip3:2181 cache.db-cache = true cache.db-cache-clean-wait = 20 cache.db-cache-time = 180000 cache.db-cache-size = 0.5 storage.hbase.table = test-graph index.test-graph.backend=elasticsearch index.test-graph.hostname=ip1:9200,ip2:9200,ip3:9200 index.test-graph.index-name = test-graph gremlin.graph = org.janusgraph.core.JanusGraphFactory #storage.hbase.ext.zookeeper.znode.parent = /hbase-unsecure //没有开kerberos时 增改以下3行配置： storage.hbase.ext.zookeeper.znode.parent = /hbase-secure storage.hbase.ext.hbase.security.authentication = kerberos storage.hbase.ext.hbase.regionserver.keytab.file = /etc/security/keytabs/hbase.service.keytab 获取Hbase用户票据（简单理解为Hbase用户认证） 执行下面的命令： kinit -kt hbase-service-keytab hbase/manager.bigdata@BIGDATA (可通过 klist 命令查看是否获取成功) 注：命令中标红的部分为USER_PRINCIPAL，不知道的话，可以通过以下命令获得： klist -kt hbase-service-keytab 配置环境变量 执行下面的命令（重要，踩坑一天）： export CLASSPATH=$CLASSPATH:/etc/hadoop/conf:/etc/hbase/conf 验证是否配置成功 path-to-janusgraph/bin下执行下面的命令打开gremlin console： ./gremlin.sh 验证是否配置成功： gremlin&gt;graph = JanusGraphFactory.open(&quot;path-to-janusgraph/conf/hadoop-graph/read-hbase.properties&quot;) 上面的命令执行成功没有报错的情况下，按照配置文件，hbase中应该生成表test-graph，可以进入hbase shell中执行 list 进行查看，若有test-graph表生成，则说明配置成功 依次执行以下命令，可以进一步验证： gremlin&gt;g = graph.traversal() gremlin&gt;mgmt = graph.openManagement() gremlin&gt;mgmt.makeVertexLabel(“person”).make() gremlin&gt;mgmt.makeEdgeLabel(“friend”).make() gremlin&gt; mgmt.makePropertyKey(&quot;name&quot;).dataType(String.class).cardinality(Cardinality.SET).make() gremlin&gt; age = mgmt.makePropertyKey(&quot;age&quot;).dataType(Integer.class).cardinality(Cardinality.SINGLE).make() gremlin&gt;ZhangSan = g.addV(“person”).property(“name”,”ZhangSan”).property(“age”,18) gremlin&gt;LiSi = g.addV(“person”).property(“name”,”LiSi”) .property(“age”,19) gremlin&gt;g.addE(“friend”).from(ZhangSan).to(LiSi) 下面的命令创建混合索引，可以在kibana的console查看，以验证ES gremlin&gt;mgmt.buildIndex(&quot;ageIndex&quot;,Vertex.class).addKey(age).buildMixedIndex(&quot;test-graph&quot;); //验证Schema、点、边等是否创建或添加成功 gremlin&gt;mgmt.printSchema() gremlin&gt;g.V().count() gremlin&gt;g.V().valueMap(true) gremlin&gt;g.E().count() gremlin&gt;mgmt.commit() gremlin&gt;g.tx().commit() ","link":"https://kelvin-qin.github.io/post/janusgraphhbasees--kerberos/"},{"title":"记一Hive执行Job挂起（Accepted状态）的异常处理","content":"问题 出现Hive执行select count(*) from ……语句时，任务长时间不运行 在Yarn界面观察任务状态一直是Accepted状态 Hive使用tez引擎启动时挂起，使用beeline连接后执行count报错 此时集群还有很多内存和CPU资源空闲！ 定位 查看Yarn UI界面的日志，有如下提示: Diagnostics:Application is added to the scheduler and is not yet activated. Queue's AM resource limit exceeded. Details : AM Partition = &lt;DEFAULT_PARTITION&gt;; AM Resource Request = &lt;memory:9216MB(9G), vCores:1&gt;; Queue Resource Limit for AM = &lt;memory:454656MB(444G), vCores:1&gt;; User AM Resource Limit of the queue = &lt;memory:229376MB(224G), vCores:1&gt;; Queue AM Resource Usage = &lt;memory:221184MB(216G), vCores:24&gt;; 说明任务被提交到调度器，但ApplicationMaster申请的资源超过了上限，剩余可用资源不足以启动AM，因此程序处于挂起状态，不能运行。 触发资源上限的可能是内存，或虚拟核数（vCores）以及二者均有,分析时可注意甄别。 解决方案 调大AM资源百分比 maximum-am-resource-percent：集群中用于运行应用程序ApplicationMaster的资源比例上限，该参数通常用于限制处于活动状态的应用程序数目。该参数类型为浮点型，我这默认是20%。所有队列的ApplicationMaster资源比例上限可通过参数yarn.scheduler.capacity. maximum-am-resource-percent设置，而单个队列可通过参数yarn.scheduler.capacity.. maximum-am-resource-percent设置适合自己的值。一般不要超过50%。 如果是HDP用户 使用Ambari修改 Maximum AM Resource 从默认的20%改到30%。 结语 这个问题不常遇到，一般只有在Yarn正在运行很多任务时出现，记录一下。但导致Yarn任务挂起，一直Accepted状态的原因不一定是这个。 为使本问题容更容易被搜索到，以上内容涉及： Application Master申请不到资源 提交了Yarn任务之后没有分配资源 Queue AM Resource Usage，Queue's AM resource limit exceeded. 修改集群用于运行应用程序ApplicationMaster的资源比例上限 yarn.scheduler.capacity.maximum-am-resource-percent调整 任务处于 Accepted 状态，但集群还有可用资源 周末愉快~ ","link":"https://kelvin-qin.github.io/post/hive-job-accepted-chu-li/"},{"title":"Spark SQL参数调优汇总|提速100%","content":"研究背景 基于TPCDS的100G，500G数据进行了99SQL综合调优测试 测试机为物理机5台，1台为管理节点，4台为计算节点 可用内存约1T，核心数（vCore）200大概 基于实践，在较大数据量下至少提速了一倍。TPCDS是一个复杂的综合任务集合，涵盖了统计分析，数据挖掘，迭代计算等各种类型的99个SQL查询，本文是针对99SQL的整体调优。 最重要的参数 一般不用调driver的相关参数,部分参数是核心缩写，使用时注意全称。 执行器个数 --num-executors 推荐物理机数的2~6倍，太大呢会增加通信和Shuffle等成本 内存分配 spark.executor.memory 也不是越大越好，大概是数据量2倍，如果是内存不足，多多益善 虚拟核心数 spark.executor.cores 推荐2的整数倍，本例单个Executor给10核 默认分区数 shuffle.partitions 或 spark.default.parallelism 这个对不同SQL影响各异，一般取线程数的2~3倍最好 SQL自动分区 auto.repartition=true 开启就行 开启堆外内存 spark.memory.offHeap.enabled &amp; spark.memory.offHeap.size=30g 开，同时要指定大小，酌情指定 内存比例 spark.memory.fraction 默认0.6，上下调一点，大概有5%的效果提升空间 存储内存比例 spark.memory.storageFraction 默认0.5，上下调节，大概有3%的效果，也可能是误差 开启推测执行 spark.speculation=true 开启，大概有5%的提升 autoBroadcastJoinThreshold spark.sql.autoBroadcastJoinThreshold=20m 调大，对某些任务类型有较大收益 其他（系统）参数 预读分区数 Blockdev=2048 这是Linux系统参数，调大，大概5%效果 使用zstd压缩 spark.io.compression.codec=zstd和spark.io.compression.zstd.level=2 以空间换时间，效果不明，或有副作用 sql.codegen spark.sql.codegen=true 效果不明显，建议开启 Shuffle缓冲大小 spark.shuffle.file.buffer=512k 没啥效果，调一下试试，万一有用 spark调度模式 spark.scheduler.mode=FAIR 默认是FIFO，凑数，有同仁说有神效，提升10倍神马的 以上反复测试了挺长时间，可以调节的参数差不多就这些，其余可以去优化一下代码写法，调优一下SQL，执行速度上还有改善空间。就说这么多，gg♨️. ","link":"https://kelvin-qin.github.io/post/spark-sql-can-shu-diao-you-hui-zong-orti-su-100/"},{"title":"Hive设置YARN执行队列","content":" 欢迎访问我的博客：https://kelvin-qzy.top/ 今天想测试一下yarn的调度模式，需要配置任务执行队列。 MapReduce引擎 设置执行任务时的yarn资源队列： 进入hive命令行后： set mapreduce.job.queuename=队列名 在启动hive时指定--hiveconf mapreduce.job.queuename=队列名 TEZ引擎 设置执行任务时的yarn资源队列： 进入Hive命令行后： set tez.queue.name=队列名 在启动hive时指定--hiveconf tez.queue.name=队列名 注意 Hive3.0默认引擎是tez，所以写mapreduce那一套指定队列不生效。 队列名的写法 示例：假设有多级队列root.xxx.yyy ⚠️队列名只需要写最后一级，写全名不识别，比如示例只需要写yyy。 切记切记！ 👀但是，Ranger的权限设置页面里，有个default的队列大家都很熟悉了，这个有时候单独写default就无效，反而得写成root.default ","link":"https://kelvin-qin.github.io/post/hive-she-zhi-yarn-zhi-xing-dui-lie/"}]}